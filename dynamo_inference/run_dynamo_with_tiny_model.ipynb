{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModel,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import time\n",
    "\n",
    "def get_transformer(model_name, return_model=True, **kwargs):\n",
    "    config, unused_kwargs = AutoConfig.from_pretrained(model_name, return_unused_kwargs=True, **kwargs)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if 'gpt2' in model_name:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    if not return_model:\n",
    "        return config, tokenizer\n",
    "    else:\n",
    "        if config.is_encoder_decoder:\n",
    "            model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "        else:\n",
    "            model = AutoModel.from_config(config)\n",
    "        return config, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config, tokenizer = get_transformer('google/mt5-small', return_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_tiny = transformers.MT5Config(\n",
    "    d_model=config.d_model//2,\n",
    "    d_kv=config.d_kv//2,\n",
    "    d_ff=config.d_ff//2,\n",
    "    num_decoder_layers=config.num_decoder_layers//2,\n",
    "    num_heads=config.num_heads//2,\n",
    "    num_layers=config.num_layers//2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tiny = AutoModelForSeq2SeqLM.from_config(config_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Generate taxonomy for query: dildo\", return_tensors=\"pt\", padding='max_length', truncation=True, max_length=50).to('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dynamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo as torchdynamo\n",
    "torchdynamo.config.cache_size_limit = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_tiny.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate2 = torchdynamo.optimize(\"inductor\")(model.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> –ú–µ—Öo≈Çudniow·Äë·Ä±·Ä¨·ÄÑ·Ä∫Î¨∏Ïù¥ ·Éò·Éõ‰∏ÄÊñπ„Åßklas Alu t√∂rt√©net t√∂rt√©net t√∂rt√©net t√∂rt√©net t√∂rt√©net t√∂rt√©net']\n"
     ]
    }
   ],
   "source": [
    "# dynamo warm up\n",
    "print(tokenizer.batch_decode(model.generate2(**inputs, min_length=15, max_length=15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 ms ¬± 140 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tokenizer.batch_decode(model.generate2(**inputs, min_length=15, max_length=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2 = tokenizer(\"Generate taxonomy for query: women gucci\", return_tensors=\"pt\", padding='max_length', truncation=True, max_length=50).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 ms ¬± 114 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tokenizer.batch_decode(model.generate2(**inputs2, min_length=15, max_length=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs3 = tokenizer(\"Generate taxonomy for query: baby milk\", return_tensors=\"pt\", padding='max_length', truncation=True, max_length=50).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.1 ms ¬± 545 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tokenizer.batch_decode(model.generate2(**inputs3, min_length=15, max_length=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>‡ªÄ‡∫Å‡∫µ‡∫î val —Å—ã–π–ª–∞„ÅÆÂäπÊûúalalanodd◊ú÷æ·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Äû·Ä¨·Ä∏„ÅÑ„Åö„Çå„ÇÇüñ±Œ∑ŒΩ–ø—Ä–∏—è—Ç–∏—è—ñ–∑–º—É◊ú÷æ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model.generate2(**inputs3, min_length=15, max_length=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>‡ªÄ‡∫Å‡∫µ‡∫î val —Å—ã–π–ª–∞„ÅÆÂäπÊûúalalanodd◊ú÷æ·Ä°·Äô·Äª·Ä≠·ÄØ·Ä∏·Äû·Ä¨·Ä∏„ÅÑ„Åö„Çå„ÇÇüñ±Œ∑ŒΩ–ø—Ä–∏—è—Ç–∏—è—ñ–∑–º—É◊ú÷æ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model.generate(**inputs3, min_length=15, max_length=15))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "class Trie(object):\n",
    "    def __init__(self, sequences: List[List[int]] = []):\n",
    "        self.trie_dict = {}\n",
    "        self.len = 0\n",
    "        if sequences:\n",
    "            for sequence in sequences:\n",
    "                Trie._add_to_trie(sequence, self.trie_dict)\n",
    "                self.len += 1\n",
    "\n",
    "        self.append_trie = None\n",
    "        self.bos_token_id = None\n",
    "\n",
    "    def append(self, trie, bos_token_id):\n",
    "        self.append_trie = trie\n",
    "        self.bos_token_id = bos_token_id\n",
    "\n",
    "    def add(self, sequence: List[int]):\n",
    "        Trie._add_to_trie(sequence, self.trie_dict)\n",
    "        self.len += 1\n",
    "\n",
    "    def get(self, prefix_sequence: List[int]):\n",
    "        return Trie._get_from_trie(\n",
    "            prefix_sequence, self.trie_dict, self.append_trie, self.bos_token_id\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_dict(trie_dict):\n",
    "        trie = Trie()\n",
    "        trie.trie_dict = trie_dict\n",
    "        trie.len = sum(1 for _ in trie)\n",
    "        return trie\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_to_trie(sequence: List[int], trie_dict: Dict):\n",
    "        if sequence:\n",
    "            if sequence[0] not in trie_dict:\n",
    "                trie_dict[sequence[0]] = {}\n",
    "            Trie._add_to_trie(sequence[1:], trie_dict[sequence[0]])\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_from_trie(\n",
    "        prefix_sequence: List[int],\n",
    "        trie_dict: Dict,\n",
    "        append_trie=None,\n",
    "        bos_token_id: int = None,\n",
    "    ):\n",
    "        if len(prefix_sequence) == 0:\n",
    "            output = list(trie_dict.keys())\n",
    "            if append_trie and bos_token_id in output:\n",
    "                output.remove(bos_token_id)\n",
    "                output += list(append_trie.trie_dict.keys())\n",
    "            return output\n",
    "        elif prefix_sequence[0] in trie_dict:\n",
    "            return Trie._get_from_trie(\n",
    "                prefix_sequence[1:],\n",
    "                trie_dict[prefix_sequence[0]],\n",
    "                append_trie,\n",
    "                bos_token_id,\n",
    "            )\n",
    "        else:\n",
    "            if append_trie:\n",
    "                return append_trie.get(prefix_sequence)\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "    def __iter__(self):\n",
    "        def _traverse(prefix_sequence, trie_dict):\n",
    "            if trie_dict:\n",
    "                for next_token in trie_dict:\n",
    "                    yield from _traverse(\n",
    "                        prefix_sequence + [next_token], trie_dict[next_token]\n",
    "                    )\n",
    "            else:\n",
    "                yield prefix_sequence\n",
    "\n",
    "        return _traverse([], self.trie_dict)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, value):\n",
    "        return self.get(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_gen_sequences = []\n",
    "with open('../modelling/datasets/taxonomy/wish_v1.2.1_newtax_leafpaths.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        l = l.replace('\\n', '').strip()\n",
    "        if len(l) > 0:\n",
    "            allowed_gen_sequences.append(l)\n",
    "\n",
    "allowed_tokids = [\n",
    "    [tokenizer.pad_token_id] + tokenizer.encode(i) + [tokenizer.eos_token_id] for i in allowed_gen_sequences\n",
    "]\n",
    "max_len = max(len(i) for i in allowed_tokids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie_fake = Trie([[0] * max_len])\n",
    "trie = Trie(allowed_tokids)\n",
    "\n",
    "def constraint_fake(batch_id, sent):\n",
    "    return trie_fake.get(sent.tolist())\n",
    "    \n",
    "def constraint(batch_id, sent):\n",
    "    return trie.get(sent.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = inputs\n",
    "batch2 = inputs2\n",
    "batch3 = inputs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate3 = torchdynamo.optimize(\"inductor\")(model.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad></s>', '<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad></s><pad>', '<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad></s><pad><pad>']\n",
      "tensor([0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "infres = model.generate3(\n",
    "    input_ids = batch[\"input_ids\"], \n",
    "    attention_mask = batch[\"attention_mask\"],\n",
    "    num_beams = 3, \n",
    "    num_return_sequences = 3, \n",
    "    do_sample = False, \n",
    "    length_penalty = 0, \n",
    "    max_new_tokens = 50 - 1, # HACK: T5 adds pad token in the beginning\n",
    "    prefix_allowed_tokens_fn=constraint_fake, # use longest fake trie to warm up\n",
    "    output_scores=True, return_dict_in_generate=True\n",
    ")\n",
    "prediction = infres.sequences\n",
    "probs = infres.sequences_scores.exp()\n",
    "print(tokenizer.batch_decode(prediction))\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.3 ms ¬± 632 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "infres = model.generate3(\n",
    "    input_ids = batch[\"input_ids\"], \n",
    "    attention_mask = batch[\"attention_mask\"],\n",
    "    num_beams = 3, \n",
    "    num_return_sequences = 3, \n",
    "    do_sample = False, \n",
    "    length_penalty = 0, \n",
    "    max_new_tokens = 50 - 1, # HACK: T5 adds pad token in the beginning\n",
    "    prefix_allowed_tokens_fn=constraint, \n",
    "    output_scores=True, return_dict_in_generate=True\n",
    ")\n",
    "prediction = infres.sequences\n",
    "probs = infres.sequences_scores.exp()\n",
    "tokenizer.batch_decode(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.2 ms ¬± 264 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "infres = model.generate3(\n",
    "    input_ids = batch2[\"input_ids\"], \n",
    "    attention_mask = batch2[\"attention_mask\"],\n",
    "    num_beams = 3, \n",
    "    num_return_sequences = 3, \n",
    "    do_sample = False, \n",
    "    length_penalty = 0, \n",
    "    max_new_tokens = 50 - 1, # HACK: T5 adds pad token in the beginning\n",
    "    prefix_allowed_tokens_fn=constraint, \n",
    "    output_scores=True, return_dict_in_generate=True\n",
    ")\n",
    "prediction = infres.sequences\n",
    "probs = infres.sequences_scores.exp()\n",
    "tokenizer.batch_decode(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.9 ms ¬± 789 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "infres = model.generate3(\n",
    "    input_ids = batch3[\"input_ids\"], \n",
    "    attention_mask = batch3[\"attention_mask\"],\n",
    "    num_beams = 3, \n",
    "    num_return_sequences = 3, \n",
    "    do_sample = False, \n",
    "    length_penalty = 0, \n",
    "    max_new_tokens = 50 - 1, # HACK: T5 adds pad token in the beginning\n",
    "    prefix_allowed_tokens_fn=constraint, \n",
    "    output_scores=True, return_dict_in_generate=True\n",
    ")\n",
    "prediction = infres.sequences\n",
    "probs = infres.sequences_scores.exp()\n",
    "tokenizer.batch_decode(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<pad> education & office supplies > cutting supplies > scissors</s><pad>',\n",
       "  '<pad> education & office supplies > cutting supplies > letter opener</s><pad>',\n",
       "  '<pad> education & office supplies > cutting supplies > utility knife</s>'],\n",
       " tensor([0., 0., 0.], device='cuda:0'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infres = model.generate3(\n",
    "    input_ids = batch3[\"input_ids\"], \n",
    "    attention_mask = batch3[\"attention_mask\"],\n",
    "    num_beams = 3, \n",
    "    num_return_sequences = 3, \n",
    "    do_sample = False, \n",
    "    length_penalty = 0, \n",
    "    max_new_tokens = 50 - 1, # HACK: T5 adds pad token in the beginning\n",
    "    prefix_allowed_tokens_fn=constraint, \n",
    "    output_scores=True, return_dict_in_generate=True\n",
    ")\n",
    "prediction = infres.sequences\n",
    "probs = infres.sequences_scores.exp()\n",
    "tokenizer.batch_decode(prediction), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<pad> education & office supplies > cutting supplies > scissors</s><pad>',\n",
       "  '<pad> education & office supplies > cutting supplies > letter opener</s><pad>',\n",
       "  '<pad> education & office supplies > cutting supplies > utility knife</s>'],\n",
       " tensor([0., 0., 0.], device='cuda:0'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infres = model.generate(\n",
    "    input_ids = batch3[\"input_ids\"], \n",
    "    attention_mask = batch3[\"attention_mask\"],\n",
    "    num_beams = 3, \n",
    "    num_return_sequences = 3, \n",
    "    do_sample = False, \n",
    "    length_penalty = 0, \n",
    "    max_new_tokens = 50 - 1, # HACK: T5 adds pad token in the beginning\n",
    "    prefix_allowed_tokens_fn=constraint, \n",
    "    output_scores=True, return_dict_in_generate=True\n",
    ")\n",
    "prediction = infres.sequences\n",
    "probs = infres.sequences_scores.exp()\n",
    "tokenizer.batch_decode(prediction), probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constrained greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate4 = torchdynamo.optimize(\"inductor\")(model.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(model.generate4(\n",
    "    input_ids = batch[\"input_ids\"], \n",
    "    attention_mask = batch[\"attention_mask\"],\n",
    "    do_sample = False, \n",
    "    length_penalty = 0, \n",
    "    max_new_tokens = 50 - 1, # HACK: T5 adds pad token in the beginning\n",
    "    prefix_allowed_tokens_fn=constraint_fake\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.2 ms ¬± 806 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tokenizer.batch_decode(model.generate4(\n",
    "    input_ids = batch[\"input_ids\"], \n",
    "    attention_mask = batch[\"attention_mask\"],\n",
    "    do_sample = False, \n",
    "    length_penalty = 0, \n",
    "    max_new_tokens = 50 - 1, # HACK: T5 adds pad token in the beginning\n",
    "    prefix_allowed_tokens_fn=constraint\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.7 ms ¬± 341 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tokenizer.batch_decode(model.generate4(\n",
    "    input_ids = batch2[\"input_ids\"], \n",
    "    attention_mask = batch2[\"attention_mask\"],\n",
    "    do_sample = False, \n",
    "    length_penalty = 0, \n",
    "    max_new_tokens = 50 - 1, # HACK: T5 adds pad token in the beginning\n",
    "    prefix_allowed_tokens_fn=constraint\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.2 ms ¬± 226 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tokenizer.batch_decode(model.generate4(\n",
    "    input_ids = batch3[\"input_ids\"], \n",
    "    attention_mask = batch3[\"attention_mask\"],\n",
    "    do_sample = False, \n",
    "    length_penalty = 0, \n",
    "    max_new_tokens = 50 - 1, # HACK: T5 adds pad token in the beginning\n",
    "    prefix_allowed_tokens_fn=constraint\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> beauty & health > shaving & hair removal > waxing</s>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model.generate4(\n",
    "    input_ids = batch3[\"input_ids\"], \n",
    "    attention_mask = batch3[\"attention_mask\"],\n",
    "    do_sample = False, \n",
    "    length_penalty = 0, \n",
    "    max_new_tokens = 50 - 1, # HACK: T5 adds pad token in the beginning\n",
    "    prefix_allowed_tokens_fn=constraint\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> beauty & health > shaving & hair removal > waxing</s>']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model.generate(\n",
    "    input_ids = batch3[\"input_ids\"], \n",
    "    attention_mask = batch3[\"attention_mask\"],\n",
    "    do_sample = False, \n",
    "    length_penalty = 0, \n",
    "    max_new_tokens = 50 - 1, # HACK: T5 adds pad token in the beginning\n",
    "    prefix_allowed_tokens_fn=constraint\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
