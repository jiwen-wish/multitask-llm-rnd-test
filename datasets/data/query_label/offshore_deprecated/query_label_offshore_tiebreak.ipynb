{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from copy import deepcopy\n",
    "from thefuzz import process as fuzz_process\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "v121_tax = pd.read_json('../../../data/taxonomy/wish_newtax.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = v121_tax[v121_tax.category_path.apply(len) > 0].category_path.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths.append(\"No Categories Match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_set = set(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_nans(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "def transform_unnamed(l):\n",
    "    l = deepcopy(l)\n",
    "    lout = []\n",
    "    for ind, i in enumerate(l):\n",
    "        if i.startswith('Unnamed:'):\n",
    "            lout.append(l[ind-1])\n",
    "            l[ind] = l[ind-1]\n",
    "        else:\n",
    "            lout.append(i)\n",
    "    return lout\n",
    "    \n",
    "def recreate_col_names(df):\n",
    "    col_names = []\n",
    "    for n1, n2 in zip(transform_unnamed(df.columns.tolist()), [transform_nans(i) for i in df.loc[0].tolist()]):\n",
    "        if len(n1) > 0 and len(n2) > 0:\n",
    "            if n1.endswith(n2):\n",
    "                n3 = n1 \n",
    "            elif n2.startswith(n1):\n",
    "                n3 = n2\n",
    "            else:\n",
    "                n3 = n1 + \" \" + n2\n",
    "        elif len(n1) > 0 and len(n2) == 0:\n",
    "            n3 = n1\n",
    "        elif len(n1) == 0 and len(n2) > 0:\n",
    "            n3 = n2\n",
    "        else:\n",
    "            raise Exception('empty columne names')\n",
    "        col_names.append(n3)\n",
    "    return col_names\n",
    "\n",
    "def rename_recreated_cols(df):\n",
    "    df.columns = recreate_col_names(df)\n",
    "    df = df.loc[1:]\n",
    "    return df\n",
    "\n",
    "def match_full_path(df, manual_correction=None):\n",
    "    recs = []\n",
    "    for i in tqdm(df.to_dict('records')):\n",
    "        for col in [ \n",
    "            'Most confident taxonomy path Full Path', \n",
    "            '2nd Most confident taxonomy path Full Path', \n",
    "            '3rd Most confident taxonomy path Full Path'\n",
    "        ]:\n",
    "            p = i[col]\n",
    "            if pd.isna(p):\n",
    "                i[col] = None\n",
    "                continue\n",
    "            if p.startswith('Retrieving data'):\n",
    "                print(p)\n",
    "                i[col] = None\n",
    "                continue\n",
    "            p = p.strip()\n",
    "            if manual_correction is not None and p in manual_correction:\n",
    "                print(f\"manually correct {p} into {manual_correction[p]}\")\n",
    "                p = manual_correction[p]\n",
    "                i[col] = p\n",
    "            if p not in paths_set:\n",
    "                match = fuzz_process.extractOne(query=p, choices=paths)\n",
    "                print(col, p, match)\n",
    "                if match[1] <= 90:\n",
    "                    print('discard')\n",
    "                    i[col] = None\n",
    "                else:\n",
    "                    print('keep')\n",
    "                    i[col] = match[0]\n",
    "        recs.append(i)\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "def collect_all_paths(df):\n",
    "    recs = []\n",
    "    for i in tqdm(df.to_dict('records')):\n",
    "        paths_collector = []\n",
    "\n",
    "        if i['Most confident taxonomy path Full Path'] == \"No Categories Match\":\n",
    "            assert pd.isna(i['2nd Most confident taxonomy path Full Path']) & \\\n",
    "                pd.isna(i['3rd Most confident taxonomy path Full Path'])\n",
    "        \n",
    "        for col in [ \n",
    "            'Most confident taxonomy path Full Path', \n",
    "            '2nd Most confident taxonomy path Full Path', \n",
    "            '3rd Most confident taxonomy path Full Path'\n",
    "        ]:\n",
    "            if not pd.isna(i[col]):\n",
    "                paths_collector.append(i[col])\n",
    "        \n",
    "        assert len(paths_collector) > 0\n",
    "        while len(paths_collector) != 3:\n",
    "            paths_collector.append(None)\n",
    "        i['All SortedByConfidenceHighestFirst taxonomy path Full Paths'] = paths_collector\n",
    "        recs.append(i)\n",
    "    return pd.DataFrame(recs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day1_labeler1 = pd.read_excel('day1/Labeller 1 - 30 Nov & 1 Dec 2022.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day1_labeler2 = pd.read_excel('day1/Labeller 2 - 30 Nov & 1 Dec 2022.xlsx', sheet_name='Query (2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day1_labeler1 = rename_recreated_cols(df_day1_labeler1)\n",
    "df_day1_labeler2 = rename_recreated_cols(df_day1_labeler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tuple(df_day1_labeler1.columns.tolist()) == tuple(df_day1_labeler2.columns.tolist())\n",
    "assert len(df_day1_labeler1) == len(df_day1_labeler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 134/780 [00:01<00:05, 124.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most confident taxonomy path Full Path toys & hobbies > puzzles & games ('Toys & Hobbies > Puzzles & Games', 100)\n",
      "keep\n",
      "Retrieving data. Wait a few seconds and try to cut or copy again.\n",
      "Retrieving data. Wait a few seconds and try to cut or copy again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 664/780 [00:02<00:00, 354.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd Most confident taxonomy path Full Path Home & Garden > Home Decor > Wall Sticker ('Home & Garden > Home Decor > Wall Stickers', 99)\n",
      "keep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [00:03<00:00, 250.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Most confident taxonomy path Full Path Watches > Men's Watches > Quartz Watch (\"Watches > Men's Watches > Quartz Watches\", 97)\n",
      "keep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_day1_labeler1 = match_full_path(df_day1_labeler1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_correction = {\n",
    "    \"Women's Clothing > Outerwear > JacketsWomen's Clothing > Outerwear > Blazers\": \"Women's Clothing > Outerwear > Blazers\",\n",
    "    \"Luggage & Bags > Coin Purses & Holders > Coin PursesLuggage & Bags > Men's Bags > Wallets\": \"Luggage & Bags > Coin Purses & Holders > Coin Purses\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 134/780 [00:01<00:05, 122.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most confident taxonomy path Full Path toys & hobbies > puzzles & games ('Toys & Hobbies > Puzzles & Games', 100)\n",
      "keep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 187/780 [00:01<00:05, 106.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most confident taxonomy path Full Path home & garden > bathroom products > bathroom gadgets > toothpaste squeezers ('Home & Garden > Bathroom Products > Bathroom Gadgets > Toothpaste Squeezers', 100)\n",
      "keep\n",
      "manually correct Women's Clothing > Outerwear > JacketsWomen's Clothing > Outerwear > Blazers into Women's Clothing > Outerwear > Blazers\n",
      "manually correct Luggage & Bags > Coin Purses & Holders > Coin PursesLuggage & Bags > Men's Bags > Wallets into Luggage & Bags > Coin Purses & Holders > Coin Purses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [00:02<00:00, 336.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Most confident taxonomy path Full Path Mother & Kids > Activity & Gear > Bouncers,Jumperms & Swings ('Mother & Kids > Activity & Gear > Bouncers,Jumpers & Swings', 99)\n",
      "keep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_day1_labeler2 = match_full_path(df_day1_labeler2, manual_correction=manual_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [00:00<00:00, 226703.42it/s]\n"
     ]
    }
   ],
   "source": [
    "df_day1_labeler1_short = collect_all_paths(df_day1_labeler1[['Sr No', 'cnt', 'gmv', 'sample_method', 'query', 'translated query (if needed)', \n",
    "    'Most confident taxonomy path Full Path', '2nd Most confident taxonomy path Full Path', \n",
    "    '3rd Most confident taxonomy path Full Path', 'Names', 'Date of Search'\n",
    "]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [00:00<00:00, 199375.78it/s]\n"
     ]
    }
   ],
   "source": [
    "df_day1_labeler2_short = collect_all_paths(df_day1_labeler2[['Sr No', 'cnt', 'gmv', 'sample_method', 'query', 'translated query (if needed)', \n",
    "    'Most confident taxonomy path Full Path', '2nd Most confident taxonomy path Full Path', \n",
    "    '3rd Most confident taxonomy path Full Path', 'Names', 'Date of Search'\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = []\n",
    "for i in df_day1_labeler1_short.columns:\n",
    "    if i in [ \n",
    "        'Most confident taxonomy path Full Path', \n",
    "        '2nd Most confident taxonomy path Full Path', \n",
    "        '3rd Most confident taxonomy path Full Path', \n",
    "        'Names',\n",
    "        'Date of Search', \n",
    "        'All SortedByConfidenceHighestFirst taxonomy path Full Paths'\n",
    "    ]:\n",
    "        col_names.append(f\"Labeler1 {i}\")\n",
    "    else:\n",
    "        col_names.append(i)\n",
    "df_day1_labeler1_short.columns = col_names\n",
    "\n",
    "col_names = []\n",
    "for i in df_day1_labeler2_short.columns:\n",
    "    if i in [ \n",
    "        'Most confident taxonomy path Full Path', \n",
    "        '2nd Most confident taxonomy path Full Path', \n",
    "        '3rd Most confident taxonomy path Full Path', \n",
    "        'Names',\n",
    "        'Date of Search', \n",
    "        'All SortedByConfidenceHighestFirst taxonomy path Full Paths'\n",
    "    ]:\n",
    "        col_names.append(f\"Labeler2 {i}\")\n",
    "    else:\n",
    "        col_names.append(i)\n",
    "df_day1_labeler2_short.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day1_merged_short = df_day1_labeler1_short.merge(df_day1_labeler2_short[[ \n",
    "    'Sr No',\n",
    "    'Labeler2 Most confident taxonomy path Full Path', \n",
    "    'Labeler2 2nd Most confident taxonomy path Full Path', \n",
    "    'Labeler2 3rd Most confident taxonomy path Full Path', \n",
    "    'Labeler2 Names',\n",
    "    'Labeler2 Date of Search', \n",
    "    'Labeler2 All SortedByConfidenceHighestFirst taxonomy path Full Paths'\n",
    "]], on=\"Sr No\", how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_day1_merged_short) == len(df_day1_labeler1_short) == len(df_day1_labeler2_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df_day1_merged_short['Labeler1 All SortedByConfidenceHighestFirst taxonomy path Full Paths'].apply(len) == 3).all()\n",
    "assert (df_day1_merged_short['Labeler2 All SortedByConfidenceHighestFirst taxonomy path Full Paths'].apply(len) == 3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day1_merged_short_agree = df_day1_merged_short[\n",
    "    df_day1_merged_short['Labeler1 All SortedByConfidenceHighestFirst taxonomy path Full Paths'].apply(tuple) == \\\n",
    "        df_day1_merged_short['Labeler2 All SortedByConfidenceHighestFirst taxonomy path Full Paths'].apply(tuple)\n",
    "]\n",
    "df_day1_merged_short_disagree = df_day1_merged_short[\n",
    "    df_day1_merged_short['Labeler1 All SortedByConfidenceHighestFirst taxonomy path Full Paths'].apply(tuple) != \\\n",
    "        df_day1_merged_short['Labeler2 All SortedByConfidenceHighestFirst taxonomy path Full Paths'].apply(tuple)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22846/669257745.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_day1_merged_short_agree['Tiebreaked All SortedByConfidenceHighestFirst taxonomy path Full Paths'] = df_day1_merged_short_agree[\n",
      "/tmp/ipykernel_22846/669257745.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_day1_merged_short_disagree['Tiebreaked All SortedByConfidenceHighestFirst taxonomy path Full Paths'] = None\n"
     ]
    }
   ],
   "source": [
    "df_day1_merged_short_agree['Tiebreaked All SortedByConfidenceHighestFirst taxonomy path Full Paths'] = df_day1_merged_short_agree[ \n",
    "    'Labeler2 All SortedByConfidenceHighestFirst taxonomy path Full Paths'\n",
    "].tolist()\n",
    "df_day1_merged_short_disagree['Tiebreaked All SortedByConfidenceHighestFirst taxonomy path Full Paths'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0858974358974359"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_day1_merged_short_agree) / len(df_day1_merged_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day1_merged_short_agree.to_json(\"day1/agree_11302022_12012022_corrected.json\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day1_merged_short_disagree.to_excel(\"day1/disagree_11302022_12012022_corrected.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
