{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/home/vscode/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_t5 = AutoTokenizer.from_pretrained('t5-base')\n",
    "tokenizer_mt5 = AutoTokenizer.from_pretrained('google/mt5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_special_tokens = tokenizer_t5.special_tokens_map['additional_special_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_special_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokenizer_mt5.tokenize(' '.join(t5_special_tokens))) == len(t5_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['▁<extra_id_99>'], ['▁<', 'extra', '_', 'id', '_100', '>'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_mt5.tokenize('<extra_id_99>'), tokenizer_mt5.tokenize('<extra_id_100>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attribute = pd.read_csv('/workspaces/multitask-llm-rnd/datasets/data/attribute_extraction_metadata_template/Initial Attribute Definition for First Release - UPDATED SHEET .csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attribute['attribute_field'].apply(lambda x: ',' in x).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attribute['attribute_field'].apply(lambda x: ':' in x).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attribute['attribute_field'].apply(lambda x: ';' in x).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "for i in df_attribute['category_attributevalue'].apply(eval).tolist():\n",
    "    for j in i:\n",
    "        if isinstance(j, list):\n",
    "            assert len(j) == 0\n",
    "        else:\n",
    "            vals.append(str(j).lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8267"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False, False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any([',' in i for i in vals]), any([';' in i for i in vals]), any([':' in i for i in vals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [ \n",
    "    '/workspaces/multitask-llm-rnd/datasets/data/wish_attr_extract_label/processed/appen_020323_030323_delivered_030623_validated_product_attr_textandimg_test.json', \n",
    "    '/workspaces/multitask-llm-rnd/datasets/data/wish_attr_extract_label/processed/appen_020323_030323_delivered_030623_validated_product_attr_textandimg_val.json', \n",
    "    '/workspaces/multitask-llm-rnd/datasets/data/wish_attr_extract_label/processed/appen_020323_030323_delivered_030623_validated_product_attr_textandimg_train.json', \n",
    "    '/workspaces/multitask-llm-rnd/datasets/data/wish_attr_extract_label/processed/appen_020323_030323_delivered_030623_validated_product_attr_textonly_test.json', \n",
    "    '/workspaces/multitask-llm-rnd/datasets/data/wish_attr_extract_label/processed/appen_020323_030323_delivered_030623_validated_product_attr_textonly_val.json', \n",
    "    '/workspaces/multitask-llm-rnd/datasets/data/wish_attr_extract_label/processed/appen_020323_030323_delivered_030623_validated_product_attr_textonly_train.json', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for fname in tqdm(fnames):\n",
    "    df = pd.read_json(fname, lines=True)\n",
    "    recs = []\n",
    "    for i in df.to_dict('records'):\n",
    "        kvdict_i = defaultdict(list)\n",
    "        for k, v in i['attr_name_value_pairs_normalized']:\n",
    "            kvdict_i[k.strip().lower()].append(v.strip().lower())\n",
    "        for k, v in i['attr_name_value_pairs_custom']:\n",
    "            kvdict_i[k.strip().lower()].append(v.strip().lower())\n",
    "        ks = list(kvdict_i)\n",
    "        np.random.shuffle(ks)\n",
    "        assert len(ks) < len(t5_special_tokens)\n",
    "        special_token_idx = 0\n",
    "        question_texts = []\n",
    "        answer_texts = []\n",
    "        for ind, k in enumerate(ks):\n",
    "            question_texts.append(f'{k}: {t5_special_tokens[special_token_idx]}')\n",
    "            if len(answer_texts) == 0:\n",
    "                answer_texts.append(f' {t5_special_tokens[special_token_idx]}')\n",
    "            vs = list(set(kvdict_i[k]))\n",
    "            np.random.shuffle(vs)\n",
    "            answer_texts.append(','.join(vs))\n",
    "            answer_texts.append(f' {t5_special_tokens[special_token_idx+1]}')\n",
    "            special_token_idx += 1\n",
    "        question_text = ';'.join(question_texts).strip()\n",
    "        answer_text = ''.join(answer_texts).strip()\n",
    "        i['attr_name_value_pairs_all_lower_t5_denoise_question'] = question_text\n",
    "        i['attr_name_value_pairs_all_lower_t5_denoise_answer'] = answer_text\n",
    "        recs.append(i)\n",
    "    df = pd.DataFrame(recs)\n",
    "    df.to_json(fname.replace('/processed/', '/processed2/').replace('.json', '_t5denoiseformat.json'), lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_ordering': 37361,\n",
       " 'sample_method': 'only_text',\n",
       " 'pid': '61a15b8d4bdcdf1db5fbd67f',\n",
       " 'category': 'Home & Garden > Home Storage & Organization > Storage Baskets',\n",
       " 'title': 'Santa,Elk,Penguin Styles Quality Hand-knitted Merry Xmas Wicker for Candy,Fruit Christmas Storage Basket Candy Box Christmas Present Table Decor',\n",
       " 'description': 'Size: S:17*12.5 cm;  L: 20*18.5 cm\\r\\nMaterial:  Wicker + cloth\\r\\nItem Name: Christmas Storage Basket\\r\\nPackaging Included: 1 * Christmas Storage Basket\\nStyle: Santa, Elk, Snowman, Penguin, Gingerbread Man\\r\\nNote:\\r\\nPlease allow a little differences due to manual measurement.\\r\\nDue to the difference between different monitors,the picture may not reflect the actual color of the item.\\r\\nThank you!',\n",
       " 'main_img_url': nan,\n",
       " 'rater_output_processed': 'Home & Garden > Home Storage & Organization > Storage Baskets > Materials > Wicker\\nHome & Garden > Home Storage & Organization > Storage Baskets > Alpha Size > L\\nHome & Garden > Home Storage & Organization > Storage Baskets > Alpha Size > S',\n",
       " 'attr_name_value_pairs_normalized': [['Alpha Size', 'L'],\n",
       "  ['Alpha Size', 'S'],\n",
       "  ['Materials', 'Wicker']],\n",
       " 'attr_name_value_pairs_custom': [],\n",
       " 'attr_name_value_pairs_normalized_text': 'Alpha Size|L\\nAlpha Size|S\\nMaterials|Wicker',\n",
       " 'attr_name_value_pairs_all_lower_t5_denoise_question': 'alpha size: <extra_id_0>;materials: <extra_id_1>',\n",
       " 'attr_name_value_pairs_all_lower_t5_denoise_answer': '<extra_id_0>l,s <extra_id_1>wicker <extra_id_2>'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁al',\n",
       " 'pha',\n",
       " '▁size',\n",
       " ':',\n",
       " '<extra_id_0>',\n",
       " '▁',\n",
       " ';',\n",
       " 'material',\n",
       " 's',\n",
       " ':',\n",
       " '<extra_id_1>']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_t5.tokenize('alpha size: <extra_id_0>;materials: <extra_id_1>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁alpha',\n",
       " '▁size',\n",
       " ':',\n",
       " '▁<extra_id_0>',\n",
       " ';',\n",
       " 'material',\n",
       " 's',\n",
       " ':',\n",
       " '▁<extra_id_1>']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_mt5.tokenize('alpha size: <extra_id_0>;materials: <extra_id_1>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁al',\n",
       " 'pha',\n",
       " '▁size',\n",
       " ':',\n",
       " '<extra_id_0>',\n",
       " '▁',\n",
       " ';',\n",
       " 'material',\n",
       " 's',\n",
       " ':',\n",
       " '<extra_id_1>']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_t5.tokenize('alpha size: <extra_id_0>;materials: <extra_id_1>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁<extra_id_0>',\n",
       " 'l',\n",
       " ',',\n",
       " 's',\n",
       " '▁<extra_id_1>',\n",
       " 'wick',\n",
       " 'er',\n",
       " '▁<extra_id_2>']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_mt5.tokenize('<extra_id_0>l,s <extra_id_1>wicker <extra_id_2>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
