{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvc.api\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_multitask_multimodal import LLM_MultitaskMultimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open('../../models/product_title_multitask_multimodal/version_1/config.yaml', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clm_multimodal_clip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding',\n",
       "     'proj_head': 'proj_head'}]}},\n",
       " 'dlm_multimodal_wishtitlewclip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding',\n",
       "     'proj_head': 'proj_head'}]}},\n",
       " 'seqclf_multimodal_wishtitlewclip2pseudov121tax': {'multimodal_embedding': {'input': [{'key': 'img_embedding',\n",
       "     'proj_head': 'proj_head'}]},\n",
       "  'specs': {'clf_head': 'clf_head',\n",
       "   'clf_weight_type': None,\n",
       "   'label_map_file': 'datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt',\n",
       "   'label_type': 'taxonomy'}},\n",
       " 'emb_singlemodal_wishquery2googletitle': None,\n",
       " 'clm_singlemodal_alititle2v121tax': None,\n",
       " 'clm_singlemodal_wishtitle2pseudov121tax': None,\n",
       " 'dlm_singlemodal_wishtitle': None,\n",
       " 'emb_singlemodal_wishtitle2pseudov121tax': None,\n",
       " 'emb_singlemodal_alititle2v121tax': None,\n",
       " 'seqclf_singlemodal_alititle2v121tax': {'specs': {'clf_head': 'clf_head',\n",
       "   'clf_weight_type': None,\n",
       "   'label_map_file': 'datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt',\n",
       "   'label_type': 'taxonomy'}},\n",
       " 'seqclf_singlemodal_wishtitle2pseudov121tax': {'specs': {'clf_head': 'clf_head',\n",
       "   'clf_weight_type': None,\n",
       "   'label_map_file': 'datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt',\n",
       "   'label_type': 'taxonomy'}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['model']['multitask_specs_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = {'clm_multimodal_clip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding',\n",
    "     'proj_head': 'proj_head'}]}},\n",
    " 'dlm_multimodal_wishtitlewclip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding',\n",
    "     'proj_head': 'proj_head'}]}},\n",
    " 'seqclf_multimodal_wishtitlewclip2pseudov121tax': {'multimodal_embedding': {'input': [{'key': 'img_embedding',\n",
    "     'proj_head': 'proj_head'}]},\n",
    "  'specs': {'clf_head': 'clf_head',\n",
    "   'clf_weight_type': None,\n",
    "   'label_map_file': '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt',\n",
    "   'label_type': 'taxonomy'}},\n",
    " 'emb_singlemodal_wishquery2googletitle': None,\n",
    " 'clm_singlemodal_alititle2v121tax': None,\n",
    " 'clm_singlemodal_wishtitle2pseudov121tax': None,\n",
    " 'dlm_singlemodal_wishtitle': None,\n",
    " 'emb_singlemodal_wishtitle2pseudov121tax': None,\n",
    " 'emb_singlemodal_alititle2v121tax': None,\n",
    " 'seqclf_singlemodal_alititle2v121tax': {'specs': {'clf_head': 'clf_head',\n",
    "   'clf_weight_type': None,\n",
    "   'label_map_file': '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt',\n",
    "   'label_type': 'taxonomy'}},\n",
    " 'seqclf_singlemodal_wishtitle2pseudov121tax': {'specs': {'clf_head': 'clf_head',\n",
    "   'clf_weight_type': None,\n",
    "   'label_map_file': '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt',\n",
    "   'label_type': 'taxonomy'}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Unused kwargs when getting t5-base: {'distance_func': 'cosine', 'loss_type': 'cross-entropy', 'margin': None, 'hidden_states_type': 'encoder-last', 'add_simcse': False, 'manual_loss_type': 'manual_mse', 'auto_task_weight': False, 'multitask_specs_dict': {'clm_multimodal_clip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding', 'proj_head': 'proj_head'}]}}, 'dlm_multimodal_wishtitlewclip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding', 'proj_head': 'proj_head'}]}}, 'seqclf_multimodal_wishtitlewclip2pseudov121tax': {'multimodal_embedding': {'input': [{'key': 'img_embedding', 'proj_head': 'proj_head'}]}, 'specs': {'clf_head': 'clf_head', 'clf_weight_type': None, 'label_map_file': '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt', 'label_type': 'taxonomy'}}, 'emb_singlemodal_wishquery2googletitle': None, 'clm_singlemodal_alititle2v121tax': None, 'clm_singlemodal_wishtitle2pseudov121tax': None, 'dlm_singlemodal_wishtitle': None, 'emb_singlemodal_wishtitle2pseudov121tax': None, 'emb_singlemodal_alititle2v121tax': None, 'seqclf_singlemodal_alititle2v121tax': {'specs': {'clf_head': 'clf_head', 'clf_weight_type': None, 'label_map_file': '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt', 'label_type': 'taxonomy'}}, 'seqclf_singlemodal_wishtitle2pseudov121tax': {'specs': {'clf_head': 'clf_head', 'clf_weight_type': None, 'label_map_file': '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt', 'label_type': 'taxonomy'}}}, 'head_dict': {'proj_head': {'type': 'linear', 'in_features': 768, 'out_features': 768, 'purpose': 'projection'}, 'clf_head': {'type': 'linear', 'in_features': 768, 'out_features': 6037, 'purpose': 'seqclf'}}}\n",
      "/opt/conda/envs/py38/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LLM_MultitaskMultimodal.load_from_checkpoint(\n",
    "    '../../models/product_title_multitask_multimodal/version_1/epoch=0-step=75000.ckpt', \n",
    "    multitask_specs_dict = md\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "tmp = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 984, 995)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attr = pd.read_csv(dvc.api.get_url(\n",
    "    'data/wish_attr_extract/Amazon_FlatCat_AttrValDef.csv',\n",
    "    repo='git@github.com:ContextLogic/multitask-llm-rnd.git'\n",
    "))\n",
    "df_attr['amazon_l1_flattype'] = (df_attr.L1_source + \" > \" + df_attr.product_type).str.lower().str.strip()\n",
    "len(set(df_attr.L1_source)), len(set(df_attr.product_type)), len(set(df_attr.amazon_l1_flattype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_amazon_l1_flattypes = list(set(df_attr['amazon_l1_flattype']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping = pd.read_excel(dvc.api.get_url(\n",
    "    'data/wish_attr_extract/Wish_Amazon_Openai.xlsx',\n",
    "    repo='git@github.com:ContextLogic/multitask-llm-rnd.git'\n",
    "), sheet_name='All Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping = df_mapping[~df_mapping.apth.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(df_mapping.wpath.apply(lambda x: x.split('/')[0].lower().strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping['amazon_leaf'] = df_mapping.apth.apply(lambda x: x.replace(\"/\", \" > \").lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_amazon_matched_leaf = list(set(df_mapping['amazon_leaf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_amazon_l1_flattypes_tensors = model.tokenizer(all_amazon_l1_flattypes, return_tensors='pt', padding=True)\n",
    "all_amazon_matched_leaf_tensors = model.tokenizer(all_amazon_matched_leaf, return_tensors='pt', padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_amazon_l1_flattypes_hidden_states = model.get_hidden_states(\n",
    "    **all_amazon_l1_flattypes_tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_amazon_matched_leaf_hidden_states = model.get_hidden_states(\n",
    "    **all_amazon_matched_leaf_tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = nn.functional.normalize(all_amazon_matched_leaf_hidden_states, 1).mm(\n",
    "    nn.functional.normalize(all_amazon_l1_flattypes_hidden_states, 1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, indices = sims.topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, i in enumerate(indices.numpy()):\n",
    "    results[all_amazon_matched_leaf[ind]] = [ \n",
    "        all_amazon_l1_flattypes[j] for j in i\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(results).T.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.columns = ['amazon_leaf'] + [f'top_{i}_pred_amz_l1flat' for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_excel('../../tmp_amazon_leaf_to_l1flat.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'amazon_leaf': 'sports & outdoors > exercise & fitness > clothing > men > sets > workout top & bottom sets',\n",
       "  'top_1_pred_amz_l1flat': 'clothing > tracksuit',\n",
       "  'top_2_pred_amz_l1flat': 'sports > sportwetsuit',\n",
       "  'top_3_pred_amz_l1flat': 'clothing > suit',\n",
       "  'top_4_pred_amz_l1flat': 'clothing > ethnicwear',\n",
       "  'top_5_pred_amz_l1flat': 'clothing > sleepwear',\n",
       "  'top_6_pred_amz_l1flat': 'clothing > jinbei',\n",
       "  'top_7_pred_amz_l1flat': 'sports > sportracket',\n",
       "  'top_8_pred_amz_l1flat': 'clothing > salwarsuitset',\n",
       "  'top_9_pred_amz_l1flat': 'clothing > luggage',\n",
       "  'top_10_pred_amz_l1flat': 'health > prescriptioneyewear'},\n",
       " {'amazon_leaf': 'sports & outdoors > clothing > men > shorts & pants > pants > snowboarding',\n",
       "  'top_1_pred_amz_l1flat': 'clothing > shorts',\n",
       "  'top_2_pred_amz_l1flat': 'clothing > pants',\n",
       "  'top_3_pred_amz_l1flat': 'clothing > tights',\n",
       "  'top_4_pred_amz_l1flat': 'clothing > underpants',\n",
       "  'top_5_pred_amz_l1flat': 'clothing > overalls',\n",
       "  'top_6_pred_amz_l1flat': 'clothing > underwear',\n",
       "  'top_7_pred_amz_l1flat': 'clothing > waistcincher',\n",
       "  'top_8_pred_amz_l1flat': 'sports > sportwetsuit',\n",
       "  'top_9_pred_amz_l1flat': 'clothing > jinbei',\n",
       "  'top_10_pred_amz_l1flat': 'clothing > bodystocking'},\n",
       " {'amazon_leaf': 'video games > legacy systems > handheld game systems > nintendo 3ds & 2ds > accessories > batteries & chargers > chargers',\n",
       "  'top_1_pred_amz_l1flat': 'softwarevideogame > videogamesaccessories',\n",
       "  'top_2_pred_amz_l1flat': 'softwarevideogame > videogames',\n",
       "  'top_3_pred_amz_l1flat': 'softwarevideogame > videogameconsole',\n",
       "  'top_4_pred_amz_l1flat': 'softwarevideogame > videogameshardware',\n",
       "  'top_5_pred_amz_l1flat': 'video > videodvd',\n",
       "  'top_6_pred_amz_l1flat': 'computers > videocard',\n",
       "  'top_7_pred_amz_l1flat': 'softwarevideogame > videogamecontroller',\n",
       "  'top_8_pred_amz_l1flat': 'computers > videoprojector',\n",
       "  'top_9_pred_amz_l1flat': 'consumerelectronics > videodiscplayer',\n",
       "  'top_10_pred_amz_l1flat': 'consumerelectronics > videodevice'},\n",
       " {'amazon_leaf': 'musical instruments > stringed instruments > folk & world > guzhengs',\n",
       "  'top_1_pred_amz_l1flat': 'musicalinstrument > stringedinstruments',\n",
       "  'top_2_pred_amz_l1flat': 'musicalinstrument > guitars',\n",
       "  'top_3_pred_amz_l1flat': 'musicalinstrument > miscworldinstruments',\n",
       "  'top_4_pred_amz_l1flat': 'musicalinstrument > instrumentpartsandaccessories',\n",
       "  'top_5_pred_amz_l1flat': 'musicalinstrument > percussioninstruments',\n",
       "  'top_6_pred_amz_l1flat': 'musicalinstrument > musicalinstruments',\n",
       "  'top_7_pred_amz_l1flat': 'musicalinstrument > brassandwoodwindinstruments',\n",
       "  'top_8_pred_amz_l1flat': 'musicalinstrument > keyboardinstruments',\n",
       "  'top_9_pred_amz_l1flat': 'health > condom',\n",
       "  'top_10_pred_amz_l1flat': 'health > sexualwellness'},\n",
       " {'amazon_leaf': 'clothing, shoes & jewelry > women > jewelry > bracelets > tennis',\n",
       "  'top_1_pred_amz_l1flat': 'jewelry > fashionnecklacebraceletanklet',\n",
       "  'top_2_pred_amz_l1flat': 'jewelry > finenecklacebraceletanklet',\n",
       "  'top_3_pred_amz_l1flat': 'jewelry > watchband',\n",
       "  'top_4_pred_amz_l1flat': 'jewelry > charm',\n",
       "  'top_5_pred_amz_l1flat': 'sports > sweatband',\n",
       "  'top_6_pred_amz_l1flat': 'jewelry > apparelpin',\n",
       "  'top_7_pred_amz_l1flat': 'shoe > sandal',\n",
       "  'top_8_pred_amz_l1flat': 'clothing > apparelbelt',\n",
       "  'top_9_pred_amz_l1flat': 'clothing > shoelace',\n",
       "  'top_10_pred_amz_l1flat': 'baby > babyproducts'}]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res.sample(5).to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ea19d11efa7602c1f12500925a974ed4f31fcf847bd6f694bd5180da2602ded"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
