{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaway: perfect long_form_product_title_text <> product_image does not mean it can be directly used for query <> product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image, ImageFile\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "# We use the original clip-ViT-B-32 for encoding images\n",
    "img_model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "# Our text embedding model is aligned to the img_model and maps 50+\n",
    "# languages to the same vector space\n",
    "text_model = SentenceTransformer('sentence-transformers/clip-ViT-B-32-multilingual-v1')\n",
    "\n",
    "\n",
    "# Now we load and encode the images\n",
    "def load_image(url_or_path):\n",
    "    if url_or_path.startswith(\"http://\") or url_or_path.startswith(\"https://\"):\n",
    "        return Image.open(requests.get(url_or_path, stream=True).raw)\n",
    "    else:\n",
    "        return Image.open(url_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_model.cuda()\n",
    "text_model.cuda()\n",
    "tmp = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import dvc.api \n",
    "\n",
    "df_clipmore_test = next(pd.read_json(dvc.api.get_url(\n",
    "    \"data/wish_clipmore/Wish_Clipmore_Tahoe_Train_Dedup.json\",\n",
    "    repo='git@github.com:ContextLogic/multitask-llm-rnd.git'\n",
    "), lines=True, chunksize=10000))\n",
    "\n",
    "df_clipmore_test['img_url'] = df_clipmore_test['product_id'].apply(\n",
    "    lambda x: f\"https://canary.contestimg.wish.com/api/webimage/{x}-large.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clipmore_test = df_clipmore_test.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We load 3 images. You can either pass URLs or\n",
    "# a path on your disc\n",
    "img_paths = df_clipmore_test['img_url'].tolist()\n",
    "\n",
    "images = [load_image(img) for img in tqdm(img_paths)]\n",
    "\n",
    "# Map images to the vector space\n",
    "img_embeddings = img_model.encode(images, show_progress_bar=True)\n",
    "\n",
    "# Now we encode our text:\n",
    "texts = df_clipmore_test.title.tolist()\n",
    "text_embeddings = text_model.encode(texts, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 3D Printed Tokyo Ghoul T-shirt Summer Mens Anime Large Size Short-sleeve T-shirt\n",
      "Score: tensor(0.3213)\n",
      "Path: https://canary.contestimg.wish.com/api/webimage/60335ab7e8cd71d755304a79-large.jpg \n",
      "\n",
      "Text: Eyebrow Trimmer with Eyebrow Comb Eyebrow Trimmer Makeup Scissors Beauty Scissors\n",
      "Score: tensor(0.2861)\n",
      "Path: https://canary.contestimg.wish.com/api/webimage/5ee85d62255ab0063d8bf67a-large.jpg \n",
      "\n",
      "Text: 925 Sterling silver Natural Malachite Peridot Oval Pendant Pure Jewelry\n",
      "Score: tensor(0.3046)\n",
      "Path: https://canary.contestimg.wish.com/api/webimage/6176b0b655a3e8e305baa5a3-large.jpg \n",
      "\n",
      "Text: Pistola Para Pintar 0.9 Litros Bdph1200- B3 Black And Decker\n",
      "Score: tensor(0.2926)\n",
      "Path: https://canary.contestimg.wish.com/api/webimage/61b74eed7e09afd6837142b4-large.jpg \n",
      "\n",
      "Text: 100Pcs/Set Ballerina Beauty Tools DIY UV Gel Coffin Fake Nails Manicure False Nail Tips Full Cover\n",
      "Score: tensor(0.3579)\n",
      "Path: https://canary.contestimg.wish.com/api/webimage/5efec170190d93004929df75-large.jpg \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute cosine similarities:\n",
    "cos_sim = util.cos_sim(text_embeddings, img_embeddings)\n",
    "c = 0\n",
    "for text, scores in zip(texts, cos_sim):\n",
    "    max_img_idx = torch.argmax(scores)\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Score:\", scores[max_img_idx] )\n",
    "    print(\"Path:\", img_paths[max_img_idx], \"\\n\")\n",
    "    c += 1\n",
    "    if c == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = df_clipmore_test['v121_category'].apply(lambda x: tuple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.078640245"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(text_embeddings, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06835164"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(img_embeddings, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.048485573"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(img_embeddings + text_embeddings, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.plot\n",
    "import umap\n",
    "umap.plot.output_notebook()\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Category20\n",
    "\n",
    "hidden_states = normalize(text_embeddings + img_embeddings)\n",
    "\n",
    "mapper = umap.UMAP().fit(hidden_states)\n",
    "proj_data = mapper.transform(hidden_states)\n",
    "\n",
    "\n",
    "output_file(\"toolbar_clip.html\")\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x=proj_data[:,0],\n",
    "            y=proj_data[:,1],\n",
    "            desc=df_clipmore_test['title'].tolist(),\n",
    "            cat=df_clipmore_test['v121_category'].apply(lambda x: \" > \".join(x)).tolist(),\n",
    "            cat_zero=df_clipmore_test['v121_category'].apply(lambda x: x[0]).tolist(),\n",
    "            imgs = df_clipmore_test['img_url'].tolist()\n",
    "        )\n",
    "    )\n",
    "\n",
    "hover = HoverTool(\n",
    "        tooltips=\"\"\"\n",
    "        <div>\n",
    "            <div>\n",
    "                <img\n",
    "                    src=\"@imgs\" height=\"100\" alt=\"@imgs\" width=\"100\"\n",
    "                    style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                    border=\"2\"\n",
    "                ></img>\n",
    "            </div>\n",
    "            <div>\n",
    "                <span style=\"font-size: 17px; font-weight: bold;\">@desc</span>\n",
    "                <span style=\"font-size: 17px; font-weight: bold;\">>>>></span>\n",
    "                <span style=\"font-size: 17px; font-weight: bold;\">@cat</span>\n",
    "                <span style=\"font-size: 15px; color: #966;\">[$index]</span>\n",
    "            </div>\n",
    "            <div>\n",
    "                <span style=\"font-size: 15px;\">Location</span>\n",
    "                <span style=\"font-size: 10px; color: #696;\">($x, $y)</span>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "p = figure(plot_width=1200, plot_height=800, tools=[hover],\n",
    "           title=\"Mouse over the dots\")\n",
    "cat0 = list(set(df_clipmore_test['v121_category'].apply(lambda x: x[0]).tolist()))\n",
    "p.circle('x', 'y', size=5, color=factor_cmap('cat_zero', palette=Category20[20], factors=cat0), source=source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manual search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06057785,  0.44280058,  0.2951861 , ...,  0.40780723,\n",
       "        -0.01515003,  0.28453058],\n",
       "       [-0.69512296, -0.2774524 ,  0.3453142 , ...,  0.38775447,\n",
       "        -0.14922294,  0.08063488],\n",
       "       [-0.858289  ,  0.2887728 , -0.11065876, ...,  0.6694223 ,\n",
       "         0.1941432 ,  0.36045936],\n",
       "       ...,\n",
       "       [-0.669177  , -0.00928982,  0.13870558, ...,  0.15982851,\n",
       "        -0.35338193, -0.03364283],\n",
       "       [-0.35870048,  0.01317909, -0.0221952 , ...,  0.7985848 ,\n",
       "        -0.62996936,  0.35887843],\n",
       "       [-0.04864804, -0.09387708, -0.19785263, ...,  0.40523928,\n",
       "         0.4519784 , -0.393081  ]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [ \n",
    "    'Black Tee',\n",
    "    'Black T shirt',\n",
    "    'White Painting',\n",
    "    'Underwear'\n",
    "]\n",
    "qs = [f'{i}' for i in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376286465cc34866918e0def97b93b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_embeddings = text_model.encode(qs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = texts\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_sim = np.vstack([bm25.get_scores(q.split(\" \")) for q in qs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Black Tee\n",
      "Title: Sold Out\n",
      "Score: tensor(1.1393, dtype=torch.float64)\n",
      "Path: https://canary.contestimg.wish.com/api/webimage/5822d0f703bcd11b5be4fea7-large.jpg \n",
      "\n",
      "Query: Black T shirt\n",
      "Title: Mom Sold My Bike Black T-shirt - Super - Motorcycle T-shirt\n",
      "Score: tensor(1.1202, dtype=torch.float64)\n",
      "Path: https://canary.contestimg.wish.com/api/webimage/59fb2cdf86ac5b2bd86219b4-large.jpg \n",
      "\n",
      "Query: White Painting\n",
      "Title: Sade White T shirt\n",
      "Score: tensor(1.0927, dtype=torch.float64)\n",
      "Path: https://canary.contestimg.wish.com/api/webimage/5e832c560e84c35e251d8f42-large.jpg \n",
      "\n",
      "Query: Underwear\n",
      "Title: King Men Casual Drawstring Joggers Sweatpants Cotton Pants 4XL\n",
      "Score: tensor(1.1035, dtype=torch.float64)\n",
      "Path: https://canary.contestimg.wish.com/api/webimage/60ae106bd81c4a382c6c3d1d-large.jpg \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cos_sim = util.cos_sim(query_embeddings, text_embeddings) + util.cos_sim(query_embeddings, img_embeddings) + 0 * bm_sim / bm_sim.max() / 8\n",
    "c = 0\n",
    "for text, scores in zip(qs, cos_sim):\n",
    "    max_img_idx = torch.argmax(scores)\n",
    "    print(\"Query:\", text)\n",
    "    print(\"Title:\", texts[max_img_idx])\n",
    "    print(\"Score:\", scores[max_img_idx] )\n",
    "    print(\"Path:\", img_paths[max_img_idx], \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ea19d11efa7602c1f12500925a974ed4f31fcf847bd6f694bd5180da2602ded"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
