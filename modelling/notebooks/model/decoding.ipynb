{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Unused kwargs when getting t5-base: {'distance_func': 'cosine', 'loss_type': 'cross-entropy', 'margin': None, 'hidden_states_type': 'encoder-last', 'add_simcse': False, 'manual_loss_type': 'manual_mse', 'auto_task_weight': False, 'multitask_specs_dict': {'clm_multimodal_clip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding', 'proj_head': 'proj_head'}]}}, 'dlm_multimodal_wishtitlewclip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding', 'proj_head': 'proj_head'}]}}, 'seqclf_multimodal_wishtitlewclip2pseudov121tax': {'multimodal_embedding': {'input': [{'key': 'img_embedding', 'proj_head': 'proj_head'}]}, 'specs': {'clf_head': 'clf_head', 'clf_weight_type': None, 'label_map_file': '../../datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt', 'label_type': 'taxonomy'}}, 'emb_singlemodal_wishquery2googletitle': None, 'clm_singlemodal_alititle2v121tax': None, 'clm_singlemodal_wishtitle2pseudov121tax': None, 'dlm_singlemodal_wishtitle': None, 'emb_singlemodal_wishtitle2pseudov121tax': None, 'emb_singlemodal_alititle2v121tax': None, 'seqclf_singlemodal_alititle2v121tax': {'specs': {'clf_head': 'clf_head', 'clf_weight_type': None, 'label_map_file': '../../datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt', 'label_type': 'taxonomy'}}, 'seqclf_singlemodal_wishtitle2pseudov121tax': {'specs': {'clf_head': 'clf_head', 'clf_weight_type': None, 'label_map_file': '../../datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt', 'label_type': 'taxonomy'}}}, 'head_dict': {'proj_head': {'type': 'linear', 'in_features': 768, 'out_features': 768, 'purpose': 'projection'}, 'clf_head': {'type': 'linear', 'in_features': 768, 'out_features': 6037, 'purpose': 'seqclf'}}}\n",
      "/opt/conda/envs/py38/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from copy import deepcopy\n",
    "import dvc.api\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from main_multitask_multimodal import LLM_MultitaskMultimodal\n",
    "import yaml\n",
    "md = {'clm_multimodal_clip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding',\n",
    "     'proj_head': 'proj_head'}]}},\n",
    " 'dlm_multimodal_wishtitlewclip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding',\n",
    "     'proj_head': 'proj_head'}]}},\n",
    " 'seqclf_multimodal_wishtitlewclip2pseudov121tax': {'multimodal_embedding': {'input': [{'key': 'img_embedding',\n",
    "     'proj_head': 'proj_head'}]},\n",
    "  'specs': {'clf_head': 'clf_head',\n",
    "   'clf_weight_type': None,\n",
    "   'label_map_file': '../../datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt',\n",
    "   'label_type': 'taxonomy'}},\n",
    " 'emb_singlemodal_wishquery2googletitle': None,\n",
    " 'clm_singlemodal_alititle2v121tax': None,\n",
    " 'clm_singlemodal_wishtitle2pseudov121tax': None,\n",
    " 'dlm_singlemodal_wishtitle': None,\n",
    " 'emb_singlemodal_wishtitle2pseudov121tax': None,\n",
    " 'emb_singlemodal_alititle2v121tax': None,\n",
    " 'seqclf_singlemodal_alititle2v121tax': {'specs': {'clf_head': 'clf_head',\n",
    "   'clf_weight_type': None,\n",
    "   'label_map_file': '../../datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt',\n",
    "   'label_type': 'taxonomy'}},\n",
    " 'seqclf_singlemodal_wishtitle2pseudov121tax': {'specs': {'clf_head': 'clf_head',\n",
    "   'clf_weight_type': None,\n",
    "   'label_map_file': '../../datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt',\n",
    "   'label_type': 'taxonomy'}}}\n",
    "model = LLM_MultitaskMultimodal.load_from_checkpoint(\n",
    "    '../../models/product_title_multitask_multimodal/version_1/epoch=0-step=75000.ckpt',\n",
    "    multitask_specs_dict = md\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_utils import Trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_gen_sequences = []\n",
    "with open('../../datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        if len(l):\n",
    "            allowed_gen_sequences.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie = Trie([\n",
    "    [model.tokenizer.pad_token_id] + model.tokenizer.encode(i) + \\\n",
    "        [model.tokenizer.eos_token_id] for i in allowed_gen_sequences\n",
    "])\n",
    "def constraint(batch_id, sent):\n",
    "    return trie.get(sent.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_excel('../eval_query/tmp_test_eval.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'tasers',\n",
       "  'y_true': \"['security & protection > self defense supplies']\",\n",
       "  'y_pred': \"['sports > roller skates, skateboards & scooters > skate shoes', 'education & office supplies > books & magazines > humanities & social science', 'sports > fitness & body building > martial clothing > martial pants']\",\n",
       "  'y_baseline': \"['security & protection > self defense supplies', 'security & protection', 'home improvement > lights & lighting > portable lighting > flashlights & torches']\"}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval[(df_eval.y_pred_jc == 0) & (df_eval.y_baseline_jc > 0)][[\n",
    "    'query', 'y_true', 'y_pred', 'y_baseline']].sample(1).to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = model.tokenizer(\"Generate taxonomy: tasers\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py38/lib/python3.8/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['<pad> sports > fitness & body building > martial clothing > martial pants</s><pad><pad>',\n",
       "  '<pad> home & garden > home textile > bedding > mattress toppers</s><pad><pad>',\n",
       "  '<pad> sports > fitness & body building > boxing > boxing trunks</s>'],\n",
       " tensor([0.7990, 0.7255, 0.7092]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = model.transformer.generate(inputs['input_ids'], num_beams=3, prefix_allowed_tokens_fn=constraint,\n",
    "    num_return_sequences=3, output_scores=True, return_dict_in_generate=True, length_penalty=1)\n",
    "model.tokenizer.batch_decode(outs.sequences), outs.sequences_scores.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<pad> sports > fitness & body building > martial clothing > martial pants</s><pad><pad>',\n",
       "  '<pad> home & garden > home textile > bedding > mattress toppers</s><pad><pad>',\n",
       "  '<pad> sports > fitness & body building > boxing > boxing trunks</s>'],\n",
       " tensor([0.0432, 0.0112, 0.0041]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = model.transformer.generate(inputs['input_ids'], num_beams=3, prefix_allowed_tokens_fn=constraint,\n",
    "    num_return_sequences=3, output_scores=True, return_dict_in_generate=True, length_penalty=0)\n",
    "model.tokenizer.batch_decode(outs.sequences), outs.sequences_scores.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ea19d11efa7602c1f12500925a974ed4f31fcf847bd6f694bd5180da2602ded"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
