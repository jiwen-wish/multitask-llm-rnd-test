{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvc.api\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_multitask_multimodal import LLM_MultitaskMultimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = yaml.safe_load(open('/workspaces/query_understanding_model/models/product_title_multitask_multimodal/version_1/config.yaml', 'r')\n",
    ")['model']['multitask_specs_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = {'clm_multimodal_clip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding',\n",
    "     'proj_head': 'proj_head'}]}},\n",
    " 'dlm_multimodal_wishtitlewclip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding',\n",
    "     'proj_head': 'proj_head'}]}},\n",
    " 'seqclf_multimodal_wishtitlewclip2pseudov121tax': {'multimodal_embedding': {'input': [{'key': 'img_embedding',\n",
    "     'proj_head': 'proj_head'}]},\n",
    "  'specs': {'clf_head': 'clf_head',\n",
    "   'clf_weight_type': None,\n",
    "   'label_map_file': '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt',\n",
    "   'label_type': 'taxonomy'}},\n",
    " 'emb_singlemodal_wishquery2googletitle': None,\n",
    " 'clm_singlemodal_alititle2v121tax': None,\n",
    " 'clm_singlemodal_wishtitle2pseudov121tax': None,\n",
    " 'dlm_singlemodal_wishtitle': None,\n",
    " 'emb_singlemodal_wishtitle2pseudov121tax': None,\n",
    " 'emb_singlemodal_alititle2v121tax': None,\n",
    " 'seqclf_singlemodal_alititle2v121tax': {'specs': {'clf_head': 'clf_head',\n",
    "   'clf_weight_type': None,\n",
    "   'label_map_file': '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt',\n",
    "   'label_type': 'taxonomy'}},\n",
    " 'seqclf_singlemodal_wishtitle2pseudov121tax': {'specs': {'clf_head': 'clf_head',\n",
    "   'clf_weight_type': None,\n",
    "   'label_map_file': '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt',\n",
    "   'label_type': 'taxonomy'}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Unused kwargs when getting t5-base: {'distance_func': 'cosine', 'loss_type': 'cross-entropy', 'margin': None, 'hidden_states_type': 'encoder-last', 'add_simcse': False, 'manual_loss_type': 'manual_mse', 'auto_task_weight': False, 'multitask_specs_dict': {'clm_multimodal_clip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding', 'proj_head': 'proj_head'}]}}, 'dlm_multimodal_wishtitlewclip2wishtitle': {'multimodal_embedding': {'input': [{'key': 'img_embedding', 'proj_head': 'proj_head'}]}}, 'seqclf_multimodal_wishtitlewclip2pseudov121tax': {'multimodal_embedding': {'input': [{'key': 'img_embedding', 'proj_head': 'proj_head'}]}, 'specs': {'clf_head': 'clf_head', 'clf_weight_type': None, 'label_map_file': '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt', 'label_type': 'taxonomy'}}, 'emb_singlemodal_wishquery2googletitle': None, 'clm_singlemodal_alititle2v121tax': None, 'clm_singlemodal_wishtitle2pseudov121tax': None, 'dlm_singlemodal_wishtitle': None, 'emb_singlemodal_wishtitle2pseudov121tax': None, 'emb_singlemodal_alititle2v121tax': None, 'seqclf_singlemodal_alititle2v121tax': {'specs': {'clf_head': 'clf_head', 'clf_weight_type': None, 'label_map_file': '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt', 'label_type': 'taxonomy'}}, 'seqclf_singlemodal_wishtitle2pseudov121tax': {'specs': {'clf_head': 'clf_head', 'clf_weight_type': None, 'label_map_file': '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt', 'label_type': 'taxonomy'}}}, 'head_dict': {'proj_head': {'type': 'linear', 'in_features': 768, 'out_features': 768, 'purpose': 'projection'}, 'clf_head': {'type': 'linear', 'in_features': 768, 'out_features': 6037, 'purpose': 'seqclf'}}}\n",
      "/opt/conda/envs/py38/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LLM_MultitaskMultimodal.load_from_checkpoint(\n",
    "    '/workspaces/query_understanding_model/models/product_title_multitask_multimodal/version_1/epoch=0-step=45000.ckpt',\n",
    "    multitask_specs_dict = md\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "label_map_file = '/workspaces/multitask-llm-rnd/modelling/datasets/taxonomy/wish_v1.2.1_newtax_allpaths.txt'\n",
    "with open(label_map_file, 'r') as f:\n",
    "    for l in f:\n",
    "        if len(l):\n",
    "            label_map[l.replace(\"\\n\", \"\")] = len(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiobotocore.credentials:Found credentials in environment variables.\n"
     ]
    }
   ],
   "source": [
    "df_clipmore_test = pd.read_json(dvc.api.get_url(\n",
    "    \"data/wish_clipmore/Wish_Clipmore_Tahoe_Test_Dedup.json\",\n",
    "    repo='git@github.com:junwang-wish/query_understanding_data.git'\n",
    "), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "model.eval()\n",
    "tmp = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demo some tasks used to train multitask v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mm_emb(item):\n",
    "    assert ('query' in item and len(item) == 1) or ('title' in item and len(item) == 1) or \\\n",
    "        ('img_embedding' in item and len(item) == 1) or \\\n",
    "        ('title' in item and 'img_embedding' in item and len(item) == 2)\n",
    "    if 'query' in item:\n",
    "        inputs = model.tokenizer(f\"Embed query: {item['query']}\", return_tensors='pt')\n",
    "        hidden_states = model.get_hidden_states(\n",
    "            input_ids = inputs['input_ids'].to(model.device), \n",
    "            attention_mask = inputs['attention_mask'].to(model.device)\n",
    "        )\n",
    "        return hidden_states.detach()\n",
    "    elif 'title' in item and 'img_embedding' in item:\n",
    "        inputs = model.tokenizer(f\"Embed product with image: [title start] {item['title']} [title end] [image start] <extra_id_99> [image end]\", return_tensors='pt')\n",
    "        input_ids = inputs['input_ids'].to(model.device)\n",
    "        img_embedding = torch.tensor([eval(item['img_embedding'])]).to(model.device)\n",
    "        proj_img_embedding = model.proj_head(img_embedding)\n",
    "        embs_swapped = torch.where(\n",
    "            (input_ids == model.local_additional_special_tokens_ids_list[-1]).unsqueeze(-1), \n",
    "            torch.zeros_like(proj_img_embedding.unsqueeze(1)),\n",
    "            model.transformer.get_input_embeddings()(input_ids)\n",
    "        )\n",
    "        attention_mask = inputs['attention_mask'].to(model.device)\n",
    "        hidden_states = model.get_hidden_states(\n",
    "            input_ids = input_ids.to(model.device), \n",
    "            attention_mask = attention_mask.to(model.device),\n",
    "            inputs_embeds = embs_swapped \n",
    "        )\n",
    "        return hidden_states.detach()\n",
    "    elif 'title' in item:\n",
    "        inputs = model.tokenizer(f\"Embed product: {item['title']}\", return_tensors='pt')\n",
    "        hidden_states = model.get_hidden_states(\n",
    "            input_ids = inputs['input_ids'].to(model.device), \n",
    "            attention_mask = inputs['attention_mask'].to(model.device)\n",
    "        )\n",
    "        return hidden_states.detach()\n",
    "    elif 'img_embedding' in item:\n",
    "        inputs = model.tokenizer(\"Embed product with image: [title start] [title end] [image start] <extra_id_99> [image end]\", return_tensors='pt')\n",
    "        input_ids = inputs['input_ids'].to(model.device)\n",
    "        img_embedding = torch.tensor([eval(item['img_embedding'])]).to(model.device)\n",
    "        proj_img_embedding = model.proj_head(img_embedding)\n",
    "        embs_swapped = torch.where(\n",
    "            (input_ids == model.local_additional_special_tokens_ids_list[-1]).unsqueeze(-1), \n",
    "            torch.zeros_like(proj_img_embedding.unsqueeze(1)),\n",
    "            model.transformer.get_input_embeddings()(input_ids)\n",
    "        )\n",
    "        attention_mask = inputs['attention_mask'].to(model.device)\n",
    "        hidden_states = model.get_hidden_states(\n",
    "            input_ids = input_ids.to(model.device), \n",
    "            attention_mask = attention_mask.to(model.device),\n",
    "            inputs_embeds = embs_swapped \n",
    "        )\n",
    "        return hidden_states.detach()\n",
    "    else:\n",
    "        raise Exception('invalid item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:28<00:00,  5.18it/s]\n"
     ]
    }
   ],
   "source": [
    "recs = []\n",
    "for i in tqdm(df_clipmore_test.to_dict('records')):\n",
    "    i['title_embedding'] = mm_emb({'title': i['title']}).cpu().tolist()[0]\n",
    "    i['titleimg_embedding'] = mm_emb({'title': i['title'], 'img_embedding': i['img_embedding']}).cpu().tolist()[0]\n",
    "    i['nullimg_embedding'] = mm_emb({'img_embedding': i['img_embedding']}).cpu().tolist()[0]\n",
    "    recs.append(deepcopy(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clipmore_test_emb = pd.DataFrame(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states1 = normalize(np.array(df_clipmore_test_emb.img_embedding.apply(eval).tolist()))\n",
    "hidden_states2 = normalize(np.array(df_clipmore_test_emb.title_embedding.tolist()))\n",
    "hidden_states3 = normalize(np.array(df_clipmore_test_emb.nullimg_embedding.tolist()))\n",
    "hidden_states4 = normalize(np.array(df_clipmore_test_emb.titleimg_embedding.tolist()))\n",
    "hidden_states5 = normalize((np.array(df_clipmore_test_emb.title_embedding.tolist()) + \\\n",
    "    np.array(df_clipmore_test_emb.img_embedding.apply(eval).tolist()))/2)\n",
    "# lab = df_clipmore_test_emb['v121_category'].apply(lambda x: tuple(x))\n",
    "lab = df_clipmore_test_emb['v121_category'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0667835589327277,\n",
       " -0.014445437032401557,\n",
       " 0.0,\n",
       " -0.025193233387842528,\n",
       " -0.015496081315965872)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(hidden_states1, lab), silhouette_score(hidden_states2, lab), \\\n",
    "    silhouette_score(hidden_states3, lab), silhouette_score(hidden_states4, lab), \\\n",
    "    silhouette_score(hidden_states5, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.plot\n",
    "import umap\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Category20\n",
    "\n",
    "hidden_states = np.array(df_clipmore_test_emb.img_embedding.apply(eval).tolist())\n",
    "# hidden_states = np.array(df_clipmore_test_emb.title_embedding.tolist())\n",
    "# hidden_states = np.array(df_clipmore_test_emb.nullimg_embedding.tolist())\n",
    "# hidden_states = np.array(df_clipmore_test_emb.titleimg_embedding.tolist())\n",
    "\n",
    "mapper = umap.UMAP().fit(hidden_states)\n",
    "proj_data = mapper.transform(hidden_states)\n",
    "\n",
    "\n",
    "output_file(\"toolbar.html\")\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x=proj_data[:,0],\n",
    "            y=proj_data[:,1],\n",
    "            desc=df_clipmore_test_emb['title'].tolist(),\n",
    "            cat=df_clipmore_test_emb['v121_category'].apply(lambda x: \" > \".join(x)).tolist(),\n",
    "            cat_zero=df_clipmore_test_emb['v121_category'].apply(lambda x: x[0]).tolist(),\n",
    "            imgs = df_clipmore_test_emb['product_id'].apply(lambda x: f\"https://canary.contestimg.wish.com/api/webimage/{x}-large.jpg\").tolist()\n",
    "        )\n",
    "    )\n",
    "\n",
    "hover = HoverTool(\n",
    "        tooltips=\"\"\"\n",
    "        <div>\n",
    "            <div>\n",
    "                <img\n",
    "                    src=\"@imgs\" height=\"100\" alt=\"@imgs\" width=\"100\"\n",
    "                    style=\"float: left; margin: 0px 15px 15px 0px;\"\n",
    "                    border=\"2\"\n",
    "                ></img>\n",
    "            </div>\n",
    "            <div>\n",
    "                <span style=\"font-size: 17px; font-weight: bold;\">@desc</span>\n",
    "                <span style=\"font-size: 17px; font-weight: bold;\">>>>></span>\n",
    "                <span style=\"font-size: 17px; font-weight: bold;\">@cat</span>\n",
    "                <span style=\"font-size: 15px; color: #966;\">[$index]</span>\n",
    "            </div>\n",
    "            <div>\n",
    "                <span style=\"font-size: 15px;\">Location</span>\n",
    "                <span style=\"font-size: 10px; color: #696;\">($x, $y)</span>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "p = figure(plot_width=1200, plot_height=800, tools=[hover],\n",
    "           title=\"Mouse over the dots\")\n",
    "cat0 = list(set(df_clipmore_test_emb['v121_category'].apply(lambda x: x[0]).tolist()))\n",
    "p.circle('x', 'y', size=5, color=factor_cmap('cat_zero', palette=Category20[20], factors=cat0), source=source)\n",
    "\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'product_id': '6180a2722b4fee63052c220c',\n",
       "  'title': 'Trend women fashion skirt newest Friday the 13th Christmas Designs 3d print fashion long sleeve teenagers casual home pajamas dress Y10',\n",
       "  'v121_category': ['mother & kids',\n",
       "   \"girls' clothing\",\n",
       "   'sleepwear & robes',\n",
       "   'nightgowns']}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_clipmore_test[['product_id', 'title', 'v121_category']].sample(1).to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://canary.contestimg.wish.com/api/webimage/6180a2722b4fee63052c220c-large.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embedding = torch.tensor([eval(df_clipmore_test[df_clipmore_test.product_id == \"6180a2722b4fee63052c220c\"][\n",
    "    ['img_embedding']].to_dict('records')[0]['img_embedding'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_img_embedding = model.proj_head(img_embedding.to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(342.6460), tensor(104.1131, device='cuda:0', grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img_embedding**2).sum(), (proj_img_embedding**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The New York City Poster Print by Panoramic Images',\n",
       "  'The Last Air Force Movie Poster Print (27 x 40) - Item # MOV1938',\n",
       "  'The Last Air Force Movie Poster Print (27 x 40) - Item # MOV193838'],\n",
       " tensor([1.1393e-07, 4.8516e-11, 4.7726e-11], device='cuda:0'),\n",
       " tensor(1.1403e-07, device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multimodal clm-based title generation with original clip emb (result is gibberish)\n",
    "label_map_rev = {label_map[i]: i for i in label_map}\n",
    "\n",
    "inputs = model.tokenizer(\"Generate title for product with image: [image start] <extra_id_99> [image end]\", return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(model.device)\n",
    "\n",
    "embs_swapped = torch.where(\n",
    "    (input_ids == model.local_additional_special_tokens_ids_list[-1]).unsqueeze(-1), \n",
    "    img_embedding.unsqueeze(1).to(model.device),\n",
    "    model.transformer.get_input_embeddings()(input_ids)\n",
    ")\n",
    "attention_mask = inputs['attention_mask'].to(model.device)\n",
    "outs = model.transformer.generate(\n",
    "        inputs_embeds=embs_swapped, attention_mask=attention_mask,\n",
    "        length_penalty=0, max_new_tokens=50, num_beams=3 ,num_return_sequences=3,\n",
    "        output_scores=True, return_dict_in_generate=True)\n",
    "model.tokenizer.batch_decode(outs.sequences, skip_special_tokens=True), outs.sequences_scores.exp(), outs.sequences_scores.exp().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([328.7288, 345.5509, 375.6101, 360.8777, 329.4005, 373.5842, 355.2832,\n",
       "        365.4133, 389.5612, 344.4916, 359.6084, 365.2524, 342.6460, 389.5612,\n",
       "        344.4916, 351.9210, 365.2524, 338.8931], device='cuda:0',\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embs_swapped[0]**2).sum(-1) # the norm of unprojected img embedding is similar to text (kinda strange..?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"New Fashion Women/Men's 3D Print Christmas Hoodies/hooded Sweatshirts + Pants Suit Clothes\",\n",
       "  \"New Fashion Women/Men's 3D Print Christmas Hoodies/hooded Sweatshirts + Pants Suits\",\n",
       "  \"New Fashion Women/Men's 3D Print Christmas Hoodies/hooded Sweatshirts + Pants Trousers Suit Clothes\"],\n",
       " tensor([4.2894e-06, 3.9345e-07, 3.4168e-07], device='cuda:0'),\n",
       " tensor(5.0245e-06, device='cuda:0'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multimodal clm-based title generation with projected clip emb (result is good)\n",
    "label_map_rev = {label_map[i]: i for i in label_map}\n",
    "\n",
    "inputs = model.tokenizer(\"Generate title for product with image: [image start] <extra_id_99> [image end]\", return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(model.device)\n",
    "\n",
    "embs_swapped = torch.where(\n",
    "    (input_ids == model.local_additional_special_tokens_ids_list[-1]).unsqueeze(-1), \n",
    "    proj_img_embedding.unsqueeze(1).to(model.device),\n",
    "    model.transformer.get_input_embeddings()(input_ids)\n",
    ")\n",
    "attention_mask = inputs['attention_mask'].to(model.device)\n",
    "outs = model.transformer.generate(\n",
    "        inputs_embeds=embs_swapped, attention_mask=attention_mask,\n",
    "        length_penalty=0, max_new_tokens=50, num_beams=3 ,num_return_sequences=3,\n",
    "        output_scores=True, return_dict_in_generate=True)\n",
    "model.tokenizer.batch_decode(outs.sequences, skip_special_tokens=True), outs.sequences_scores.exp(), outs.sequences_scores.exp().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([328.7288, 345.5509, 375.6101, 360.8777, 329.4005, 373.5842, 355.2832,\n",
       "        365.4133, 389.5612, 344.4916, 359.6084, 365.2524, 104.1131, 389.5612,\n",
       "        344.4916, 351.9210, 365.2524, 338.8931], device='cuda:0',\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embs_swapped[0]**2).sum(-1) # strange that the norm of projected img embedding is much lower than text, is it normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Christmas Hoodie',\n",
       "  \"New Fashion Women/Men's 3D Print Christmas Hoodies/hooded Sweatshirts\",\n",
       "  \"New Fashion Women/Men's 3D Print Christmas Hoodies/hooded Sweatshirts + Pants Suit Clothes\"],\n",
       " tensor([1.6350e-03, 4.1582e-06, 9.6984e-07], device='cuda:0'),\n",
       " tensor(0.0016, device='cuda:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multimodal dlm-based title denoising\n",
    "label_map_rev = {label_map[i]: i for i in label_map}\n",
    "\n",
    "inputs = model.tokenizer(\"Denoise product with image: [title start] <extra_id_0> [title end] [image start] <extra_id_99> [image end]\", return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(model.device)\n",
    "\n",
    "embs_swapped = torch.where(\n",
    "    (input_ids == model.local_additional_special_tokens_ids_list[-1]).unsqueeze(-1), \n",
    "    proj_img_embedding.unsqueeze(1),\n",
    "    model.transformer.get_input_embeddings()(input_ids)\n",
    ")\n",
    "attention_mask = inputs['attention_mask'].to(model.device)\n",
    "outs = model.transformer.generate(\n",
    "        inputs_embeds=embs_swapped, attention_mask=attention_mask,\n",
    "        length_penalty=0, max_new_tokens=50, num_beams=3 ,num_return_sequences=3,\n",
    "        output_scores=True, return_dict_in_generate=True)\n",
    "model.tokenizer.batch_decode(outs.sequences, skip_special_tokens=True), outs.sequences_scores.exp(), outs.sequences_scores.exp().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad><extra_id_0> X-Mas<extra_id_1></s><pad><pad><pad><pad><pad><pad>',\n",
       " '<pad><extra_id_0> X-Mas -<extra_id_1></s><pad><pad><pad><pad>',\n",
       " '<pad><extra_id_0> X-Mas X-Mas<extra_id_1></s>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multimodal dlm-based title denoising with image zeroed\n",
    "label_map_rev = {label_map[i]: i for i in label_map}\n",
    "\n",
    "inputs = model.tokenizer(\"Denoise product with image: [title start] <extra_id_0> [title end] [image start] <extra_id_99> [image end]\", return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(model.device)\n",
    "\n",
    "embs_swapped = torch.where(\n",
    "    (input_ids == model.local_additional_special_tokens_ids_list[-1]).unsqueeze(-1), \n",
    "    torch.zeros_like(proj_img_embedding.unsqueeze(1)),\n",
    "    model.transformer.get_input_embeddings()(input_ids)\n",
    ")\n",
    "attention_mask = inputs['attention_mask'].to(model.device)\n",
    "model.tokenizer.batch_decode(\n",
    "    model.transformer.generate(\n",
    "            inputs_embeds=embs_swapped, attention_mask=attention_mask,\n",
    "            length_penalty=0, max_new_tokens=50, num_beams=3 ,num_return_sequences=3,\n",
    "            do_sample=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"women's clothing\", 0.9481745362281799),\n",
       " (\"women's clothing > underwear & sleepwear > women's sleepwear\",\n",
       "  0.22060589492321014),\n",
       " (\"women's clothing > matching sets\", 0.11320054531097412),\n",
       " (\"women's clothing > underwear & sleepwear\", 0.07634633034467697),\n",
       " ('mother & kids', 0.06752103567123413),\n",
       " (\"women's clothing > underwear & sleepwear > women's sleepwear > pajama sets\",\n",
       "  0.032819561660289764),\n",
       " (\"women's clothing > bottoms\", 0.02738446369767189),\n",
       " ('mother & kids > matching family outfits', 0.027149299159646034),\n",
       " (\"women's clothing > matching sets > skirt sets\", 0.023866329342126846),\n",
       " (\"women's clothing > underwear & sleepwear > women's sleepwear > sleep tops\",\n",
       "  0.01636957749724388)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multimodal seqclf-based categorize product\n",
    "label_map_rev = {label_map[i]: i for i in label_map}\n",
    "\n",
    "inputs = model.tokenizer(\"Classify product with image: [title start] Trend women fashion skirt newest Friday the 13th Christmas Designs 3d print fashion long sleeve teenagers casual home pajamas dress Y10 [title end] [image start] <extra_id_99> [image end]\", return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(model.device)\n",
    "\n",
    "embs_swapped = torch.where(\n",
    "    (input_ids == model.local_additional_special_tokens_ids_list[-1]).unsqueeze(-1), \n",
    "    proj_img_embedding.unsqueeze(1),\n",
    "    model.transformer.get_input_embeddings()(input_ids)\n",
    ")\n",
    "attention_mask = inputs['attention_mask'].to(model.device)\n",
    "hidden_states = model.get_hidden_states(\n",
    "    input_ids = input_ids.to(model.device), \n",
    "    attention_mask = attention_mask.to(model.device),\n",
    "    inputs_embeds = embs_swapped \n",
    ")\n",
    "\n",
    "logits = model.clf_head(hidden_states)\n",
    "prediction = logits\n",
    "probs = prediction.sigmoid()\n",
    "top_probs, top_pred_indices = probs.topk(probs.size(1))\n",
    "assert prediction.size(1) == len(label_map)\n",
    "\n",
    "[(label_map_rev[i], probs[0][i].item()) for i in top_pred_indices.detach().cpu().numpy().reshape(-1)[:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"women's clothing\", 0.9443934559822083),\n",
       " (\"women's clothing > underwear & sleepwear > women's sleepwear\",\n",
       "  0.24849344789981842),\n",
       " (\"women's clothing > underwear & sleepwear\", 0.11866361647844315),\n",
       " (\"women's clothing > matching sets\", 0.08950916677713394),\n",
       " ('mother & kids', 0.057061802595853806),\n",
       " (\"women's clothing > bottoms\", 0.05022074282169342),\n",
       " (\"women's clothing > matching sets > skirt sets\", 0.033552687615156174),\n",
       " ('home & garden', 0.021680690348148346),\n",
       " (\"women's clothing > underwear & sleepwear > women's sleepwear > pajama sets\",\n",
       "  0.02051149122416973),\n",
       " ('mother & kids > matching family outfits', 0.016158943995833397)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multimodal seqclf-based categorize product with image zeroed\n",
    "label_map_rev = {label_map[i]: i for i in label_map}\n",
    "\n",
    "inputs = model.tokenizer(\"Classify product with image: [title start] Trend women fashion skirt newest Friday the 13th Christmas Designs 3d print fashion long sleeve teenagers casual home pajamas dress Y10 [title end] [image start] <extra_id_99> [image end]\", return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(model.device)\n",
    "\n",
    "embs_swapped = torch.where(\n",
    "    (input_ids == model.local_additional_special_tokens_ids_list[-1]).unsqueeze(-1), \n",
    "    torch.zeros_like(proj_img_embedding.unsqueeze(1)),\n",
    "    model.transformer.get_input_embeddings()(input_ids)\n",
    ")\n",
    "attention_mask = inputs['attention_mask'].to(model.device)\n",
    "hidden_states = model.get_hidden_states(\n",
    "    input_ids = input_ids.to(model.device), \n",
    "    attention_mask = attention_mask.to(model.device),\n",
    "    inputs_embeds = embs_swapped \n",
    ")\n",
    "\n",
    "logits = model.clf_head(hidden_states)\n",
    "prediction = logits\n",
    "probs = prediction.sigmoid()\n",
    "top_probs, top_pred_indices = probs.topk(probs.size(1))\n",
    "assert prediction.size(1) == len(label_map)\n",
    "\n",
    "[(label_map_rev[i], probs[0][i].item()) for i in top_pred_indices.detach().cpu().numpy().reshape(-1)[:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"women's clothing > matching sets > skirt sets\",\n",
       "  'mother & kids > pregnancy & maternity > maternity clothing > sleep & lounge',\n",
       "  \"women's clothing > matching sets > dress sets\"],\n",
       " tensor([0.6870, 0.0868, 0.0785], device='cuda:0'),\n",
       " tensor(0.8523, device='cuda:0'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multimodal clm-based categorize product\n",
    "label_map_rev = {label_map[i]: i for i in label_map}\n",
    "\n",
    "inputs = model.tokenizer(\"Generate taxonomy for product with image: [title start] Trend women fashion skirt newest Friday the 13th Christmas Designs 3d print fashion long sleeve teenagers casual home pajamas dress Y10 [title end] [image start] <extra_id_99> [image end]\", return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(model.device)\n",
    "\n",
    "embs_swapped = torch.where(\n",
    "    (input_ids == model.local_additional_special_tokens_ids_list[-1]).unsqueeze(-1), \n",
    "    proj_img_embedding.unsqueeze(1),\n",
    "    model.transformer.get_input_embeddings()(input_ids)\n",
    ")\n",
    "attention_mask = inputs['attention_mask'].to(model.device)\n",
    "outs = model.transformer.generate(\n",
    "        inputs_embeds=embs_swapped, attention_mask=attention_mask,\n",
    "        length_penalty=0, max_new_tokens=50, num_beams=3 ,num_return_sequences=3,\n",
    "        output_scores=True, return_dict_in_generate=True)\n",
    "model.tokenizer.batch_decode(outs.sequences, skip_special_tokens=True), outs.sequences_scores.exp(), outs.sequences_scores.exp().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"women's clothing > matching sets > skirt sets\",\n",
       "  'mother & kids > pregnancy & maternity > maternity clothing > sleep & lounge',\n",
       "  \"women's clothing > underwear & sleepwears > women's sleepwears > sleep tops\"],\n",
       " tensor([0.5756, 0.1099, 0.0932], device='cuda:0'),\n",
       " tensor(0.7787, device='cuda:0'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multimodal clm-based categorize product with image zeroed\n",
    "label_map_rev = {label_map[i]: i for i in label_map}\n",
    "\n",
    "inputs = model.tokenizer(\"Generate taxonomy for product with image: [title start] Trend women fashion skirt newest Friday the 13th Christmas Designs 3d print fashion long sleeve teenagers casual home pajamas dress Y10 [title end] [image start] <extra_id_99> [image end]\", return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(model.device)\n",
    "\n",
    "embs_swapped = torch.where(\n",
    "    (input_ids == model.local_additional_special_tokens_ids_list[-1]).unsqueeze(-1), \n",
    "    torch.zeros_like(proj_img_embedding.unsqueeze(1)),\n",
    "    model.transformer.get_input_embeddings()(input_ids)\n",
    ")\n",
    "attention_mask = inputs['attention_mask'].to(model.device)\n",
    "outs = model.transformer.generate(\n",
    "        inputs_embeds=embs_swapped, attention_mask=attention_mask,\n",
    "        length_penalty=0, max_new_tokens=50, num_beams=3 ,num_return_sequences=3,\n",
    "        output_scores=True, return_dict_in_generate=True)\n",
    "model.tokenizer.batch_decode(outs.sequences, skip_special_tokens=True), outs.sequences_scores.exp(), outs.sequences_scores.exp().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black clothes\n",
      "0.18413197994232178\n",
      "--------------------\n",
      "pink clothes\n",
      "0.23975355923175812\n",
      "--------------------\n",
      "white clothes\n",
      "0.2434498518705368\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# multimodal search by embedding (not working now, need discussion on loss function)\n",
    "inputs = model.tokenizer(\"Embed image: [title start] Clothes [title end] [image start] <extra_id_99> [image end]\", return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(model.device)\n",
    "\n",
    "embs_swapped = torch.where(\n",
    "    (input_ids == model.local_additional_special_tokens_ids_list[-1]).unsqueeze(-1), \n",
    "    proj_img_embedding.unsqueeze(1),\n",
    "    model.transformer.get_input_embeddings()(input_ids)\n",
    ")\n",
    "\n",
    "attention_mask = inputs['attention_mask'].to(model.device)\n",
    "hidden_states_product = model.get_hidden_states(\n",
    "    input_ids = input_ids.to(model.device), \n",
    "    attention_mask = attention_mask.to(model.device),\n",
    "    inputs_embeds = embs_swapped \n",
    ")\n",
    "\n",
    "qs = ['black clothes', 'pink clothes', 'white clothes']\n",
    "for q in qs:\n",
    "    print(q)\n",
    "    inputs = model.tokenizer(f\"Embed query: {q}\", return_tensors='pt')\n",
    "    hidden_states_q = model.get_hidden_states(\n",
    "        input_ids = inputs['input_ids'].to(model.device), \n",
    "        attention_mask = inputs['attention_mask'].to(model.device)\n",
    "    )\n",
    "    print(nn.functional.normalize(hidden_states_q).mm(nn.functional.normalize(hidden_states_product).T).item())\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqclf-based categorize query\n",
    "label_map_rev = {label_map[i]: i for i in label_map}\n",
    "\n",
    "inputs = model.tokenizer(\"Classify query: pillow\", return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "hidden_states = model.get_hidden_states(\n",
    "    input_ids = input_ids.to(model.device), \n",
    "    attention_mask = attention_mask.to(model.device)\n",
    ")\n",
    "logits = model.clf_head(hidden_states)\n",
    "prob_nums = logits.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = prob_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('home & garden > home textile', 0.8359919190406799),\n",
       " ('home & garden', 0.6865457892417908),\n",
       " ('home & garden > home textile > bedding', 0.38045328855514526),\n",
       " ('home & garden > home textile > bedding > pillow cases',\n",
       "  0.24303345382213593),\n",
       " ('home & garden > home textile > pillows', 0.06814387440681458),\n",
       " ('home & garden > home textile > pillows > decorative pillows',\n",
       "  0.031411249190568924),\n",
       " ('automobiles & motorcycles > interior accessories > neck pillow',\n",
       "  0.022724634036421776),\n",
       " ('toys & hobbies > stuffed animals & plush toys > plush pillows',\n",
       "  0.020035220310091972),\n",
       " ('automobiles & motorcycles', 0.01586918905377388),\n",
       " ('home & garden > home textile > table & sofa linens > cushion covers',\n",
       "  0.015179386362433434)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_probs, top_pred_indices = probs.topk(probs.size(1))\n",
    "assert prediction.size(1) == len(label_map)\n",
    "\n",
    "[(label_map_rev[i], probs[0][i].item()) for i in top_pred_indices.detach().cpu().numpy().reshape(-1)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['home & garden > home textile > pillows > decorative pillows',\n",
       "  'home & garden > home textile > bedding > pillow cases',\n",
       "  'home & garden > home textile > table & sofa linens > cushion'],\n",
       " tensor([0.4519, 0.1555, 0.0466], device='cuda:0'),\n",
       " tensor(0.6540, device='cuda:0'))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clm-based categorize query\n",
    "inputs = model.tokenizer(\"Generate taxonomy for query: pillow\", return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(model.device)\n",
    "attention_mask = inputs['attention_mask'].to(model.device)\n",
    "outs = model.transformer.generate(\n",
    "        input_ids=input_ids, attention_mask=attention_mask,\n",
    "        length_penalty=0., max_new_tokens=50, num_beams=3 ,num_return_sequences=3,\n",
    "        output_scores=True, return_dict_in_generate=True)\n",
    "model.tokenizer.batch_decode(outs.sequences, skip_special_tokens=True), outs.sequences_scores.exp(), outs.sequences_scores.exp().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline for query classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(dvc.api.get_url(\n",
    "    'data/query/wish_queries_inferred_newtax.json',\n",
    "    repo='git@github.com:junwang-wish/query_understanding_data.git'\n",
    "), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tax = pd.read_json(dvc.api.get_url(\n",
    "    'data/taxonomy/wish_newtax.json',\n",
    "    repo='git@github.com:junwang-wish/query_understanding_data.git'\n",
    "), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_official = {}\n",
    "for i in df_tax.to_dict('records'):\n",
    "    label_map_official[int(i['id'])] = i['category_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_paths'] = df['categories'].apply(lambda x: [label_map_official[int(i)].lower().strip() for i in x.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'cheap womens shirts size small',\n",
       "  'norm': 1.0392304659,\n",
       "  'categories': '6058,6057,6023',\n",
       "  'category_names': \"Women's Tops,Muslim Fashion,Women's Clothing\",\n",
       "  'weights': '1.9245008973,0.38490017946,0.38490017946',\n",
       "  'category_paths': [\"women's clothing > muslim fashion > women's tops\",\n",
       "   \"women's clothing > muslim fashion\",\n",
       "   \"women's clothing\"]},\n",
       " {'query': 'grabadora voz',\n",
       "  'norm': 1.0392304659,\n",
       "  'categories': '1692,1689,1495',\n",
       "  'category_names': 'Digital Voice Recorder,Portable Audio & Video,Consumer Electronics',\n",
       "  'weights': '1.9245008973,0.38490017946,0.38490017946',\n",
       "  'category_paths': ['consumer electronics > portable audio & video > digital voice recorder',\n",
       "   'consumer electronics > portable audio & video',\n",
       "   'consumer electronics']}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2).to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clm_query(q):\n",
    "    # clm-based categorize query\n",
    "    inputs = model.tokenizer(f\"Generate taxonomy for query: {q}\", return_tensors='pt')\n",
    "    input_ids = inputs['input_ids'].to(model.device)\n",
    "    attention_mask = inputs['attention_mask'].to(model.device)\n",
    "    outs = model.transformer.generate(\n",
    "            input_ids=input_ids, attention_mask=attention_mask,\n",
    "            length_penalty=0., max_new_tokens=50, num_beams=3 ,num_return_sequences=3,\n",
    "            output_scores=True, return_dict_in_generate=True)\n",
    "    return model.tokenizer.batch_decode(outs.sequences, skip_special_tokens=True), \\\n",
    "        outs.sequences_scores.exp().detach().cpu().numpy(), \\\n",
    "            outs.sequences_scores.exp().sum().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqclf_query(q):\n",
    "    # seqclf-based categorize query\n",
    "    inputs = model.tokenizer(f\"Classify query: {q}\", return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    hidden_states = model.get_hidden_states(\n",
    "        input_ids = input_ids.to(model.device), \n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "    )\n",
    "    logits = model.clf_head(hidden_states)\n",
    "    probs = logits.sigmoid()\n",
    "    top_probs, top_pred_indices = probs.topk(probs.size(1))\n",
    "    assert prediction.size(1) == len(label_map)\n",
    "\n",
    "    return [(label_map_rev[i], probs[0][i].item()) for i in top_pred_indices.detach().cpu().numpy().reshape(-1)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df.sample(20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['clm_query'] = df_tmp['query'].apply(clm_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp[['query', 'category_paths', 'clm_query']].to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To beat query baseline (a strong one since it is based directly on noisy Wish customer implicit feedback), we should exploit such implicit feedback too, but not get affected by the noise\n",
    "2. For seqclf, is there a better decoding method than picking top-K predicted taxonomy paths (could be L1, L2, ...)? For example, add up the logits to leaf or a certain level with logit > lambda\n",
    "3. Similarly for clm, is there a better decoding method than beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top reasons T5 model is bad at query classification\n",
    "1. Non-English query --> Translate\n",
    "2. Miss-spelling in query --> Data augmentation?\n",
    "3. Query is \"X for Y\", but model classifies Y's category instead --> More labelled data for training\n",
    "4. Query is \"adj. + X\", which changes X's category, but model gets fixated on X's category --> More labelled data for training\n",
    "5. Isoteric brand name --> More labelled data for training\n",
    "6. Consumer electrical / hand tools --> More labelled data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((['computer & office > servers',\n",
       "   'computer & office > computer components > motherboards',\n",
       "   'computer & office > computer components > internal hard drives'],\n",
       "  array([0.12470548, 0.12196591, 0.07991078], dtype=float32),\n",
       "  array(0.32658216, dtype=float32)),\n",
       " [('computer & office', 0.5845221281051636),\n",
       "  ('home improvement', 0.296642929315567),\n",
       "  ('automobiles & motorcycles', 0.1690264493227005),\n",
       "  ('computer & office > computer components', 0.11630456149578094),\n",
       "  ('home improvement > hardware', 0.053344424813985825),\n",
       "  ('automobiles & motorcycles > auto replacement parts', 0.04588837921619415),\n",
       "  ('home improvement > electrical equipments & supplies', 0.01563027687370777),\n",
       "  ('automobiles & motorcycles > auto replacement parts > cooling system',\n",
       "   0.014263961464166641),\n",
       "  ('automobiles & motorcycles > auto replacement parts > engines & components',\n",
       "   0.01367215532809496),\n",
       "  ('computer & office > laptop parts', 0.012038576416671276)])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '3090 gpu'\n",
    "clm_query(q), seqclf_query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', 'מטען', '▁', 'למברג', '▁bo', 'sch', '▁18', 'v']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.tokenize(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really bad mistakes\n",
    "bad = ['jesus rhinestone hat', 'activia t', 'accessori gabbie criceti',\n",
    "'מטען למברג bosch 18v', 'pince à dénuder', 'iphnoe 14', 'aparelho auditivo generic',\n",
    "'mercedes e klasse 2004auße spigel', 'valkoinen kauluspaita'\n",
    "]\n",
    "# really good corrections\n",
    "good = ['panello solare ta ventola']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'jesus rhinestone hat',\n",
       "  'category_paths': [\"apparel accessories > women's hats > women's baseball caps\",\n",
       "   \"apparel accessories > women's hats\",\n",
       "   'apparel accessories'],\n",
       "  'clm_query': ([\"apparel accessories > women's hats > women's fedoras\",\n",
       "    \"apparel accessories > men's hats > men's fedoras\",\n",
       "    \"apparel accessories > women's hats > women's baseball caps\"],\n",
       "   array([0.41313443, 0.12854807, 0.10324986], dtype=float32),\n",
       "   array(0.6449324, dtype=float32))},\n",
       " {'query': 'activia t',\n",
       "  'category_paths': ['home & garden > garden supplies > pest control > repellents',\n",
       "   'home improvement > lights & lighting > portable lighting > flashlights & torches',\n",
       "   'home & garden > garden supplies > pest control',\n",
       "   'home improvement'],\n",
       "  'clm_query': (['education & office supplies > books & magazines > humanities & social science',\n",
       "    'education & office supplies > books & magazines > literature & fiction',\n",
       "    'automobiles & motorcycles > car wash & maintenance > paint care > paint protective foil'],\n",
       "   array([0.52192295, 0.01459598, 0.00288734], dtype=float32),\n",
       "   array(0.5394063, dtype=float32))},\n",
       " {'query': 'valkoinen kauluspaita',\n",
       "  'category_paths': [\"men's clothing > shirts > dress shirts\",\n",
       "   \"men's clothing > shirts\",\n",
       "   \"men's clothing\"],\n",
       "  'clm_query': ([\"apparel accessories > men's accessories > men's masks\",\n",
       "    \"apparel accessories > men's hats > men's fedoras\",\n",
       "    'beauty & health > hair care & styling > hair color'],\n",
       "   array([0.23794472, 0.06435795, 0.06218657], dtype=float32),\n",
       "   array(0.36448926, dtype=float32))},\n",
       " {'query': 'accessori gabbie criceti',\n",
       "  'category_paths': ['home & garden > pet products > reptile & amphibian supplies > feeding & watering supplies',\n",
       "   'home & garden > pet products',\n",
       "   'home & garden'],\n",
       "  'clm_query': (['home improvement > electrical equipments & supplies > motors & parts > electricity generation',\n",
       "    'home improvement > hardware > furniture hardware > wire hole covers',\n",
       "    'home improvement > electrical equipments & supplies > motors & parts > motor controller'],\n",
       "   array([0.02890035, 0.02230307, 0.00381328], dtype=float32),\n",
       "   array(0.0550167, dtype=float32))},\n",
       " {'query': 'mercedes e klasse 2004auße spigel',\n",
       "  'category_paths': ['automobiles & motorcycles > car lights > signal lamp',\n",
       "   'automobiles & motorcycles > car lights',\n",
       "   'automobiles & motorcycles'],\n",
       "  'clm_query': (['automobiles & motorcycles > interior accessories > automotive interior stickers',\n",
       "    'automobiles & motorcycles > auto replacement parts > exterior parts > emblems',\n",
       "    'automobiles & motorcycles > interior accessories > interior mouldings'],\n",
       "   array([0.20025794, 0.10850224, 0.08025032], dtype=float32),\n",
       "   array(0.38901052, dtype=float32))},\n",
       " {'query': 'aparelho auditivo generic',\n",
       "  'category_paths': ['beauty & health > health care > household health monitors > hearing aids',\n",
       "   'beauty & health > health care > household health monitors',\n",
       "   'beauty & health > health care'],\n",
       "  'clm_query': (['computer & office > mouse & keyboards > keyboards',\n",
       "    'computer & office > networking > powerline network adapters',\n",
       "    'computer & office > mouse & keyboards > mice'],\n",
       "   array([0.05555733, 0.02672331, 0.02338783], dtype=float32),\n",
       "   array(0.10566847, dtype=float32))},\n",
       " {'query': 'iphnoe 14',\n",
       "  'category_paths': ['cellphones & telecommunications > phone bags & cases > phone pouches',\n",
       "   'cellphones & telecommunications > phone bags & cases',\n",
       "   'cellphones & telecommunications'],\n",
       "  'clm_query': (['computer & office > servers',\n",
       "    'computer & office > computer components > computer cases & towers',\n",
       "    'automobiles & motorcycles > car repair tools > diagnostic tools > software'],\n",
       "   array([0.16675116, 0.02745677, 0.01873832], dtype=float32),\n",
       "   array(0.21294625, dtype=float32))},\n",
       " {'query': 'pince à dénuder',\n",
       "  'category_paths': ['tools > hand tools > pliers',\n",
       "   'tools > hand tools',\n",
       "   'tools'],\n",
       "  'clm_query': (['beauty & health > nails art & tools > nail tools > dotting tools',\n",
       "    'jewelry & accessories > jewelry sets & more > brooches',\n",
       "    'jewelry & accessories > jewelry making > jewelry findings & components'],\n",
       "   array([0.11455329, 0.06629232, 0.05381702], dtype=float32),\n",
       "   array(0.23466262, dtype=float32))},\n",
       " {'query': 'מטען למברג bosch 18v',\n",
       "  'category_paths': ['consumer electronics > accessories & parts > chargers',\n",
       "   'consumer electronics > accessories & parts',\n",
       "   'consumer electronics'],\n",
       "  'clm_query': (['tools > power tools > electric drills',\n",
       "    'home improvement > home appliances > major appliances > water heaters > gas water heaters',\n",
       "    'home improvement > electrical equipments & supplies > generators > generator parts & accessories'],\n",
       "   array([0.13955778, 0.02325021, 0.01563665], dtype=float32),\n",
       "   array(0.17844464, dtype=float32))}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp[df_tmp['query'].apply(lambda x: x in bad)][['query', 'category_paths', 'clm_query']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ea19d11efa7602c1f12500925a974ed4f31fcf847bd6f694bd5180da2602ded"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
